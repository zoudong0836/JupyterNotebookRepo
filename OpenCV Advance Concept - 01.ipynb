{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#特征检测\" data-toc-modified-id=\"特征检测-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>特征检测</a></span><ul class=\"toc-item\"><li><span><a href=\"#Harris-角点检测\" data-toc-modified-id=\"Harris-角点检测-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Harris 角点检测</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#亚像素级精确度的角点\" data-toc-modified-id=\"亚像素级精确度的角点-1.1.0.1\"><span class=\"toc-item-num\">1.1.0.1&nbsp;&nbsp;</span>亚像素级精确度的角点</a></span></li></ul></li><li><span><a href=\"#Shi-Tomasi-角点检测\" data-toc-modified-id=\"Shi-Tomasi-角点检测-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Shi-Tomasi 角点检测</a></span></li></ul></li><li><span><a href=\"#SIFT(Scale-invariant-feature-transform)\" data-toc-modified-id=\"SIFT(Scale-invariant-feature-transform)-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>SIFT(Scale-invariant feature transform)</a></span><ul class=\"toc-item\"><li><span><a href=\"#SIFT算法特点\" data-toc-modified-id=\"SIFT算法特点-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span><a href=\"https://blog.csdn.net/g11d111/article/details/79925827\" target=\"_blank\">SIFT算法特点</a></a></span></li><li><span><a href=\"#SIFT算法的步骤\" data-toc-modified-id=\"SIFT算法的步骤-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>SIFT算法的步骤</a></span><ul class=\"toc-item\"><li><span><a href=\"#尺度空间极值检测\" data-toc-modified-id=\"尺度空间极值检测-1.2.2.1\"><span class=\"toc-item-num\">1.2.2.1&nbsp;&nbsp;</span>尺度空间极值检测</a></span></li><li><span><a href=\"#关键点定位\" data-toc-modified-id=\"关键点定位-1.2.2.2\"><span class=\"toc-item-num\">1.2.2.2&nbsp;&nbsp;</span>关键点定位</a></span></li><li><span><a href=\"#方向确定\" data-toc-modified-id=\"方向确定-1.2.2.3\"><span class=\"toc-item-num\">1.2.2.3&nbsp;&nbsp;</span>方向确定</a></span></li><li><span><a href=\"#关键点描述\" data-toc-modified-id=\"关键点描述-1.2.2.4\"><span class=\"toc-item-num\">1.2.2.4&nbsp;&nbsp;</span>关键点描述</a></span></li></ul></li><li><span><a href=\"#SIFT函数及示例\" data-toc-modified-id=\"SIFT函数及示例-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>SIFT函数及示例</a></span></li></ul></li><li><span><a href=\"#FAST(Features-from-accelerated-segment-test)\" data-toc-modified-id=\"FAST(Features-from-accelerated-segment-test)-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>FAST(Features from accelerated segment test)</a></span><ul class=\"toc-item\"><li><span><a href=\"#实现步骤:\" data-toc-modified-id=\"实现步骤:-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span><a href=\"https://segmentfault.com/a/1190000015731543\" target=\"_blank\">实现步骤:</a></a></span></li><li><span><a href=\"#总结：\" data-toc-modified-id=\"总结：-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>总结：</a></span></li><li><span><a href=\"#FAST函数及示例\" data-toc-modified-id=\"FAST函数及示例-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>FAST函数及示例</a></span></li></ul></li><li><span><a href=\"#BRIEF(Binary-Robust-Independent-Elementary-Features)\" data-toc-modified-id=\"BRIEF(Binary-Robust-Independent-Elementary-Features)-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>BRIEF(Binary Robust Independent Elementary Features)</a></span><ul class=\"toc-item\"><li><span><a href=\"#算法步骤\" data-toc-modified-id=\"算法步骤-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>算法步骤</a></span></li><li><span><a href=\"#示例：\" data-toc-modified-id=\"示例：-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>示例：</a></span></li><li><span><a href=\"#总结：\" data-toc-modified-id=\"总结：-1.4.3\"><span class=\"toc-item-num\">1.4.3&nbsp;&nbsp;</span>总结：</a></span></li></ul></li><li><span><a href=\"#ORB(Oriented-FAST-and-Rotated-BRIEF)\" data-toc-modified-id=\"ORB(Oriented-FAST-and-Rotated-BRIEF)-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>ORB(Oriented FAST and Rotated BRIEF)</a></span><ul class=\"toc-item\"><li><span><a href=\"#示例\" data-toc-modified-id=\"示例-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span><a href=\"https://github.com/zibuyu1995/ApplicationInImageProcessing/tree/master/ORB-imageSearch\" target=\"_blank\">示例</a></a></span></li></ul></li></ul></li><li><span><a href=\"#特征匹配\" data-toc-modified-id=\"特征匹配-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>特征匹配</a></span><ul class=\"toc-item\"><li><span><a href=\"#Brute-Force(蛮力匹配)\" data-toc-modified-id=\"Brute-Force(蛮力匹配)-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span><a href=\"https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_matcher/py_matcher.html\" target=\"_blank\">Brute-Force(蛮力匹配)</a></a></span></li></ul></li><li><span><a href=\"#机器学习\" data-toc-modified-id=\"机器学习-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>机器学习</a></span><ul class=\"toc-item\"><li><span><a href=\"#K近邻(k-Nearest-Neighbour)\" data-toc-modified-id=\"K近邻(k-Nearest-Neighbour)-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>K近邻(k-Nearest Neighbour)</a></span><ul class=\"toc-item\"><li><span><a href=\"#KNN算法原理\" data-toc-modified-id=\"KNN算法原理-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span><a href=\"https://blog.csdn.net/App_12062011/article/details/52153878\" target=\"_blank\">KNN算法原理</a></a></span></li><li><span><a href=\"#kNN函数及示例\" data-toc-modified-id=\"kNN函数及示例-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>kNN函数及示例</a></span></li></ul></li><li><span><a href=\"#SVN(Support-Vector-Machines)支持向量机\" data-toc-modified-id=\"SVN(Support-Vector-Machines)支持向量机-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>SVN(Support Vector Machines)支持向量机</a></span></li><li><span><a href=\"#K均值算法(K-Means)\" data-toc-modified-id=\"K均值算法(K-Means)-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>K均值算法(K-Means)</a></span><ul class=\"toc-item\"><li><span><a href=\"#K均值算法的实现过程：\" data-toc-modified-id=\"K均值算法的实现过程：-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>K均值算法的实现过程：</a></span></li><li><span><a href=\"#K均值算法的缺陷：\" data-toc-modified-id=\"K均值算法的缺陷：-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>K均值算法的缺陷：</a></span></li><li><span><a href=\"#函数原型及示例\" data-toc-modified-id=\"函数原型及示例-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>函数原型及示例</a></span></li></ul></li></ul></li><li><span><a href=\"#视频分析\" data-toc-modified-id=\"视频分析-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>视频分析</a></span><ul class=\"toc-item\"><li><span><a href=\"#Meanshift\" data-toc-modified-id=\"Meanshift-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span><a href=\"https://blog.csdn.net/xiao__run/article/details/77135375\" target=\"_blank\">Meanshift</a></a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征检测\n",
    "> 粗略地讲，特征就是有意义的图像区域，该区域具有独特性或易于识别性。因此，角点及高密度区域是很好的特征，而大量重复的模式或低密度区域则不是好的特征。  \n",
    "> 例如边缘可以将图像分成两个区域，可以看作好的特征；斑点(与周围有很大差别的图像区域)也是有意义的特征  \n",
    "\n",
    ">> OpenCVZ中常见的特征检测和提取算法：\n",
    "* Harris: 该算法用于检测角点\n",
    "* SIFT: 该算法用于检测斑点(blob)\n",
    "* SURF: 该算法用于检测斑点\n",
    "* FAST: 该算法用于检测角点\n",
    "* BRIEF: 该算法用于检测斑点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harris 角点检测\n",
    "> 角点的一个特性：向任何方向移动变化都很大  \n",
    "> Harris 角点检测的结果是一个由角点分数构成的灰度图像。选取适当的阈值对结果图像进行二值化就检测到了图像中的角点。\n",
    "\n",
    "> 函数原型：\n",
    "```python\n",
    "cornerHarris(src, blockSize, ksize, k[, dst[, borderType]]) -> dst\n",
    "```\n",
    "\n",
    "> 参数说明:\n",
    "```python\n",
    "* src： 单通道8位或浮点型源图片\n",
    "* blockSize： 角点检测中要考虑的领域大小(int)\n",
    "* ksize： Sobel求导中使用的窗口大小(int)；该参数定义了角点检测的敏感度，其取值必须是介于3和31之间的奇数\n",
    "* k： Harris角点检测方程中的自由参数，取值参数为 [0.04，0.06](double)\n",
    "* borderType： 边框类型，可选参数\n",
    "```\n",
    "\n",
    "> 返回值说明:\n",
    "```python\n",
    "* dst: 一个浮点型图像，大小跟输入相同(灰度图像坐标与原图像对应)，表示Harris角点检测的结果，浮点值越高，表示越可能是角点\n",
    "```\n",
    "\n",
    "> 示例:\n",
    "```python\n",
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread('chessboard.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # 1. Harris角点检测基于灰度图像\n",
    "dst = cv2.cornerHarris(gray, 2, 3, 0.04)      # 2. Harris角点检测\n",
    "img[dst > 0.01 * dst.max()] = [0, 0, 255]     # 3. 角点标记为红色（阈值可动态调整）\n",
    "cv2.imshow('dst', img)\n",
    "cv2.waitKey()\n",
    "```\n",
    "\n",
    "> 小结:\n",
    "* Harris角点检测是一种基于灰度图像的角点检测方法\n",
    "* Harris具有旋转不变特性，即使图片发生了旋转，也能找到同样的角点\n",
    "* 其原理是先选取一个矩形块，移动这个矩形块，然后计算像素间的差值\n",
    "\n",
    "#### 亚像素级精确度的角点\n",
    "> OpenCV 提供了函数 cv2.cornerSubPix() 用于亚像素级别的角点检测  \n",
    "> 首先要找到Harris角点，然后将角点的重心传给这个函数进行修正; 在使用这个函数时要定义一个迭代停止条件。当迭代次数达到或者精度条件满足后迭代就会停止\n",
    "\n",
    "> 函数原型：\n",
    "```python\n",
    "cornerSubPix(image, corners, winSize, zeroZone, criteria) -> corners\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shi-Tomasi 角点检测\n",
    "> Shi-Tomasi算法是对Harris角点检测的改进；在图像中寻找具有大特征值的角点  \n",
    "> Shi-Tomasi算法获取图像中N个最好的角点，直接返回角点的坐标值corners (函数会采用角点质量最高的那个角点（排序后的第一个），然后将它附近（最小距离之内）的角点都删掉。按着这样的方式最后返回 N 个最佳角点)\n",
    "\n",
    "> 函数原型：\n",
    "```python\n",
    "goodFeaturesToTrack(image, maxCorners, qualityLevel, minDistance[, corners[, mask[, blockSize[, useHarrisDetector[, k]]]]]) -> corners\n",
    "```\n",
    "\n",
    "> 函数说明:\n",
    "```\n",
    "* 算法首先进行角点检测，类似Harris算法，返回一个角点检测的浮点型响应结果，浮点值越高，越有可能是角点  \n",
    "* 其次需要指定最多找多少个角点，也就是第2个参数`maxCorners`\n",
    "* 然后输入阈值参数`qualityLevel`，范围在0~1之间，这个值跟Harris算法中的阈值0.01×dst.max()含义相同，可以理解成角点的概率，只有大于这个阈值的点才会被认为是角点  \n",
    "* 最后再指定角点间的最短欧式距离`minDistance`  \n",
    "```\n",
    "\n",
    "> 参数说明:\n",
    "```python\n",
    "* maxCorners :  最大数目的角点数  \n",
    "* qualityLevel： 该参数指出最低可接受的角点质量，是一个百分数。具体来说，如果最好的角点质量=1500，而qualityLevel = 0.01，那么角点质量<15的就都会被拒掉\n",
    "* minDistance： 角点之间最小的欧拉距离，对于初选出的角点而言，如果在其周围minDistance范围内存在其他更强角点，则将此角点删除    \n",
    "* corners： 检测到的角点的输出向量  \n",
    "* mask： 可选参数，给出ROI。该参数与原图尺寸相同且类型为CV_8UC1，指示出需要进行特征检测的区域，如在人脸检测中检测到的人脸区域进行特征点提取\n",
    "* blocksize：在每一个像素的领域中计算derivative covariation matrix的平均块的大小。即计算M用到的块大小  \n",
    "```\n",
    "\n",
    "> 示例:\n",
    "```python\n",
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(\"../img/blox.jpg\")\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "corners = cv2.goodFeaturesToTrack(gray, 20, 0.1, 10)   # 试图寻找20个角点(返回结果<=20)，并且角点之间的距离不小于10个像素； 返回值 corners 是一个三维ndarray [20, 1, 3]\n",
    "corners = np.int0(corners)\n",
    "for i in corners:\n",
    "    x, y = i.ravel()     # 扁平化操作，压缩到一维\n",
    "    cv2.circle(img, (x, y), 4, (0, 0, 255), -1)\n",
    "cv2.imshow('dst', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT(Scale-invariant feature transform)\n",
    "> 也叫尺度不变特征变换算法，是David Lowe于1999年提出的局部特征描述子（Descriptor）；Sift特征匹配算法可以处理两幅图像之间发生平移、旋转、仿射变换情况下的匹配问题，具有很强的匹配能力  \n",
    "> SIFT 特征是基于物体上的一些局部外观的兴趣点而与影像的大小和旋转无关。对于光线、噪声、些微视角改变的容忍度也相当高。基于这些特性，它们是高度显著而且相对容易撷取，在母数庞大的特征数据库中，很容易辨识物体而且鲜有误认  \n",
    "> 其应用范围包含物体辨识、机器人地图感知与导航、影像缝合、3D模型建立、手势辨识、影像追踪和动作比对\n",
    "\n",
    "### [SIFT算法特点](https://blog.csdn.net/g11d111/article/details/79925827)\n",
    "> * `Sift特征是图像的局部特征` 对平移、旋转、尺度缩放、亮度变化、遮挡和噪声等具有良好的不变性，对视觉变化、仿射变换也保持一定程度的稳定性\n",
    "> * `独特性好` 信息量丰富，适用于在海量特征数据库中进行快速、准确的匹配\n",
    "> * `多量性` 即使少数的几个物体也可以产生大量Sift特征向量\n",
    "> * `速度相对较快` 优化的Sift匹配算法甚至可以达到实时的要求\n",
    "> * `可扩展性强` 可以很方便的与其他形式的特征向量进行联合\n",
    ">> SIFT算法的实质是：“不同的尺度空间上查找关键点(特征点)，并计算出关键点的方向”，SIFT所查找到的关键点是一些十分突出，不会因光照，仿射变换和噪音等因素而变化的点，如角点、边缘点、暗区的亮点及亮区的暗点等。\n",
    "\n",
    "### SIFT算法的步骤\n",
    "\n",
    "#### 尺度空间极值检测\n",
    "> 搜索所有尺度上的图像位置。通过高斯微分函数来识别潜在的对于尺度和旋转不变的兴趣点   \n",
    "> 尺度空间的获取需要使用尺度空间滤波（scale-space filtering）来实现，已证明高斯卷积核是实现尺度变换的唯一变换核，并且是唯一的线性核。尺度规范化的LoG(Laplacion of Gaussian)算子具有真正的尺度不变性，但是由于LoG算法复杂度较高。因此，使用更为高效的高斯差分算子（Difference of Gaussians）近似LoG算子来进行极值检测  \n",
    "> 当得到DoG（Difference of Gaussian）之后，图像在尺度空间中搜寻局部极值（local extrema）。图像中的某个像素点不但与其附近的8个像素点比较，而且与其前一层（previous scale）的9个像素点和下一层（next scale）的9个像素点进行比较（需为同一Octave）。如果该像素点是局部极值点，那么就认为它是一个潜在的KeyPoint（关键点）--最能代表这个scale的点  \n",
    "\n",
    "#### 关键点定位\n",
    "> 在每个候选的位置上，通过一个拟合精细的模型来确定位置和尺度。关键点的选择依据于它们的稳定程度  \n",
    "> 由第①步检测得到的极值点是离散空间的极值点。通过拟合三维二次函数来精确确定关键点的位置和尺度，同时去除`低对比度的关键点`和`不稳定的边缘响应点`(因为DoG算子会产生较强的边缘响应)，以增强匹配稳定性、提高抗噪声能力 \n",
    "\n",
    "#### 方向确定\n",
    "> 基于图像局部的梯度方向，分配给每个关键点位置一个或多个方向。所有后面的对图像数据的操作都相对于关键点的方向、尺度和位置进行变换，从而提供对于这些变换的不变性  \n",
    "\n",
    "#### 关键点描述\n",
    "> 在每个关键点周围的邻域内，在选定的尺度上测量图像局部的梯度。这些梯度被变换成一种表示，这种表示允许比较大的局部形状的变形和光照变化  \n",
    "\n",
    "### SIFT函数及示例\n",
    "> 函数原型:\n",
    "```python\n",
    "detectAndCompute(self, image, mask, descriptors=None, useProvidedKeypoints=None)  \n",
    "cv2.drawKeypoints(image, keypoints, outImage, color=None, flags=None)\n",
    "```\n",
    "\n",
    "> [示例:](https://www.jianshu.com/p/f1b97dacc501)\n",
    "```python\n",
    "def sift_kp(image):       # SIFT特征点和特征描述的提取\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.xfeatures2d_SIFT.create()     # sift = cv2.xfeatures2d.SIFT_create()\n",
    "    kp,des = sift.detectAndCompute(image, None)\n",
    "    kp_image = cv2.drawKeypoints(gray_image, kp, None)\n",
    "    return kp_image,kp,des\n",
    "```\n",
    "```python\n",
    "def get_good_match(des1,des2):    # SIFT特征点的匹配\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "    good = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.75 * n.distance:\n",
    "            good.append(m)\n",
    "    return good\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAST(Features from accelerated segment test)\n",
    "> 一种用于角点检测的算法，该算法的原理是取图像中检测点，以该点为圆心的周围的16个像素点判断检测点是否为角点，通俗的讲就是中心的的像素值比大部分周围的像素值要亮一个阈值或者暗一个阈值则为角点.\n",
    "\n",
    "### [实现步骤:](https://segmentfault.com/a/1190000015731543)\n",
    "> * 一个以像素p为中心，半径为3的圆上，有16个像素点（p1、p2、...、p16）  \n",
    "> * 定义一个阈值，计算p1、p9与中心p的像素差，若它们绝对值都小于阈值，则p点不可能是特征点，直接pass掉，否则，当做候选点  \n",
    "> * 若p是候选点，则计算p1、p9、p5、p13与中心p的像素差，若它们的绝对值有至少3个超过阈值，则当做候选点，否则，直接pass掉  \n",
    "> * 若p是候选点，则计算p1到p16这16个点与中心p的像素差，若它们有至少9个超过阈值，则是特征点，否则，直接pass掉  \n",
    "> * 对图像进行非极大值抑制：计算特征点出的FAST得分值（即score值，也即s值），判断以特征点p为中心的一个邻域（如3x3或5x5）内，计算若有多个特征点，则判断每个特征点的s值（16个点与中心差值的绝对值总和），若p是邻域所有特征点中响应值最大的，则保留；否则，抑制。若邻域内只有一个特征点（角点），则保留  \n",
    "\n",
    "### 总结：\n",
    "> * FAST 算法比其它角点检测算法都快    \n",
    "> * 但是在噪声很高时不够稳定，这是由阈值决定的  \n",
    "\n",
    "### FAST函数及示例\n",
    "> 函数原型:\n",
    "```python\n",
    "FastFeatureDetector_create(threshold=None, nonmaxSuppression=None, type=None)  \n",
    "detect(self, image, mask=None)  \n",
    "```\n",
    "\n",
    "> [示例:](https://segmentfault.com/a/1190000015731543)\n",
    "```python\n",
    "img = cv2.imread('../img/blox.jpg')\n",
    "fast = cv2.FastFeatureDetector_create()  # Initiate FAST object with default values\n",
    "kp = fast.detect(img,None)   # find and draw the keypoints\n",
    "img2 = cv2.drawKeypoints(img, kp, None, color=(255,0,0))\n",
    "print \"Threshold: {}\".format(fast.getThreshold())   # Threshold: 10\n",
    "print \"nonmaxSuppression:{}\".format(fast.getNonmaxSuppression())  # nonmaxSuppression:True\n",
    "print \"neighborhood: {}\".format(fast.getType())  # neighborhood: 2\n",
    "print \"Total Keypoints with nonmaxSuppression: {}\".format(len(kp))  # Total Keypoints with nonmaxSuppression: 431\n",
    "cv2.imshow('fast_true',img2)\n",
    "fast.setNonmaxSuppression(0)  # Disable nonmaxSuppression\n",
    "kp = fast.detect(img,None)\n",
    "print \"Total Keypoints without nonmaxSuppression: {}\".format(len(kp))  # Total Keypoints without nonmaxSuppression: 1575\n",
    "img3 = cv2.drawKeypoints(img, kp, None, color=(255,0,0))\n",
    "cv2.imshow('fast_false',img3)\n",
    "cv2.waitKey()\n",
    "```\n",
    "```python\n",
    "img = cv2.imread('1.jpg',0)\n",
    "fast=cv2.FastFeatureDetector_create(threshold=20, nonmaxSuppression=True, type=cv2.FAST_FEATURE_DETECTOR_TYPE_9_16)   #获取FAST角点探测器\n",
    "kp=fast.detect(img, None) #描述符\n",
    "img = cv2.drawKeypoints(img, kp, img, color=(255,0,0)) #画到img上面\n",
    "print \"Threshold: \", fast.getThreshold()  #输出阈值\n",
    "print \"nonmaxSuppression: \", fast.getNonmaxSuppression()  #是否使用非极大值抑制\n",
    "print \"Total Keypoints with nonmaxSuppression: \", len(kp)  #特征点个数\n",
    "cv2.imshow('sp', img)\n",
    "cv2.waitKey(0)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRIEF(Binary Robust Independent Elementary Features)\n",
    "> BRIEF 是一种对特征点描述符计算和匹配的快速方法。这种算法可以实现很高的识别率，除非出现平面内的大旋转  \n",
    "> BRIEF 不提供查找特征的方法。所以得使用其他特征检测器，比如 SIFT 和 FAST 等\n",
    "\n",
    "### 算法步骤\n",
    "> * 为减少噪声干扰，先对图像进行高斯滤波（方差为2，高斯窗口为9x9）\n",
    "> * 以特征点为中心，取SxS的邻域大窗口。在大窗口中随机选取一对（两个）5x5的子窗口，比较子窗口内的像素和（可用积分图像完成），进行二进制赋值.（一般S=31）\n",
    "> * 在大窗口中随机选取 N 对子窗口，重复步骤2的二进制赋值，形成一个二进制编码，这个编码就是对特征点的描述，即特征描述子.（一般N=256）\n",
    "\n",
    "### 示例：\n",
    ">``` python\n",
    "img = cv2.imread('../img/home.jpg', 0)\n",
    "star = cv2.xfeatures2d.StarDetector_create()   # Initiate STAR detector\n",
    "brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()  # Initiate BRIEF extractor\n",
    "kp = star.detect(img, None)         # find the keypoints with STAR\n",
    "kp, des = brief.compute(img, kp)    # compute the descriptors with BRIEF\n",
    "print brief.descriptorSize()        # 32\n",
    "print des.shape                     # (224L, 32L)\n",
    "```\n",
    "```python\n",
    "img = cv2.imread('../img/home.jpg', 0)\n",
    "fast = cv2.FastFeatureDetector_create()   # Initiate FAST detector\n",
    "brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()  # Initiate BRIEF extractor\n",
    "kp = fast.detect(img, None)         # find the keypoints with STAR\n",
    "kp, des = brief.compute(img, kp)    # compute the descriptors with BRIEF\n",
    "print brief.descriptorSize()        # 32\n",
    "print des.shape                     # (4463L, 32L)\n",
    "```\n",
    "\n",
    "### 总结：\n",
    "BRIEF的优点在于速度，但缺点也相当明显：\n",
    "* 不具备旋转不变性\n",
    "* 对噪声敏感\n",
    "* 不具备尺度不变性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORB(Oriented FAST and Rotated BRIEF)\n",
    "\n",
    "> ORB 是一个在计算量和匹配性能以及专利问题上替代SIFT和SURF算法的一个算法  \n",
    "\n",
    "> ORB 基本是 FAST 关键点检测和 BRIEF 关键点描述器的结合体，并通过很多修改增强了性能。首先它使用 FAST 找到关键点，然后再使用 Harris角点检测对这些关键点进行排序找到其中的前 N 个点。它也使用金字塔从而产生尺度不变性特征  \n",
    "\n",
    "> 由于 FAST 算法不计算方向，为了解决旋转不变性，其使用灰度矩的算法计算出角点的方向。以角点到角点所在（小块）区域质心的方向为向量的方向。为了进一步提高旋转不变性，要计算以角点为中心半径为 r 的圆形区域的矩，再根据矩计算除方向。\n",
    "\n",
    "> 对于描述符， ORB 使用的是 BRIEF 描述符。BRIEF对与旋转是不稳定的，所以，ORB根据特征点的方向使用了一种“可控BRIEF”描述符。\n",
    "\n",
    "\n",
    "### [示例](https://github.com/zibuyu1995/ApplicationInImageProcessing/tree/master/ORB-imageSearch)\n",
    "```python\n",
    "img1 = cv2.imread('test1.png')\n",
    "img2 = cv2.imread('test12.png')\n",
    "orb = cv2.ORB_create(5000)\n",
    "kp1, des1 = orb.detectAndCompute(img1,None)\n",
    "kp2, des2 = orb.detectAndCompute(img2,None)\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING)    # 提取并计算特征点\n",
    "matches = bf.knnMatch(des1, trainDescriptors = des2, k = 2)   # knn筛选结果\n",
    "good = [m for (m,n) in matches if m.distance < 0.75*n.distance]\n",
    "print len(good)   # 查看最大匹配点数目\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征匹配"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Brute-Force(蛮力匹配)](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_matcher/py_matcher.html)\n",
    "> 蛮力匹配器是很简单的。首先在第一幅图像中选取一个关键点然后依次与第二幅图像的每个关键点进行（描述符）距离测试，最后返回距离最近的关键点\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K近邻(k-Nearest Neighbour)\n",
    "> K近邻算法（KNN，K-NearestNeighbors）是一种非常简单的机器学习方法，它既可以处理分类问题，也可以处理回归问题，而且它的执行效果非常好。 \n",
    "\n",
    "> KNN是一种懒惰学习算法（lazy learningalgorithm）。所谓懒惰算法指的是，直到有了新的测试样本，该算法才开始依据训练样本进行样本的预测处理工作，也就是说该算法事先不会对训练样本进行任何的处理，只会“懒散”的等待测试样本的到来，然后才开始工作；所以它需要更大的存储空间用于存储训练样本数据。总之，懒惰学习算法非常适用于那些需要处理具有较少特征属性的大型数据库的问题。  \n",
    "\n",
    "### [KNN算法原理](https://blog.csdn.net/App_12062011/article/details/52153878)\n",
    "> 在训练样本集中，每个样本都是一个具有n个特征属性的向量，即x = (x1,x2,…,xn)，因此可以认为每个样本在n维特征空间，或度量空间内分布。每个样本还有一个唯一属于它的标签y，机器学习的目的就是找到这样的一个函数f，使y=f(x)，这样当有一个新的样本u时，我们就可以通过该目标函数确定它的标签。  \n",
    "\n",
    "> 既然样本可以被认为是在度量空间内分布，那么就可以用距离测度来衡量它们的相似程度。常用的距离测度包括欧氏距离和曼哈顿距离，以及更一般的明氏距离（这三个距离测度只适用于特征属性是连续变量的情况，当特征属性是离散变量时，如对文本进行分类，就需要用汉明距离）  \n",
    "\n",
    "> KNN的任务就是在训练样本集中，依据距离测度找到与测试样本u最相似的那K个训练样本x。对于分类问题，采用“多数表决”的方式来确定u的最终分类，即这K个训练样本中，哪个分类的样本数多，u就属于哪个分类  \n",
    "\n",
    "> 对于KNN来说，有一个最重要的参数需要事先确定，那就是K值。选择不同的K值，最终的预测结果可能会不同。K值选取的过小，会引入误差，而K值过大，虽然更准确，但会使在特征空间内明确的边界变得模糊。因此K值既要足够的大，以保证预测结果的正确性，又要足够的小，以使K个训练样本与测试样本具有一定的相似性。目前选择K值比较常用的方法是交叉验证方法（cross-validation） \n",
    "\n",
    "> 需要说明的是，OpenCV并没有采用交叉验证和距离权值的方法\n",
    "\n",
    "> 不足之处：对于一个测试数据集，需要测量它到每一个样本的距离，从而根据最近邻居分类；但是测量所有的距离需要足够的时间，并且需要大量的内存存储训练样本\n",
    "\n",
    "### kNN函数及示例\n",
    "> kNN函数:\n",
    "```python\n",
    "knn = cv2.ml.KNearest_create()\n",
    "knn.train(trainData, cv2.ml.ROW_SAMPLE, responses)\n",
    "ret, results, neighbours, dist = knn.findNearest(newcomer, 7)\n",
    "```\n",
    "\n",
    "> [示例:](https://segmentfault.com/a/1190000015731543)\n",
    "```python\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "trainData = np.random.randint(0, 100, (25, 2)).astype(np.float32)  # Feature set containing (x,y) values of 25 known/training data\n",
    "responses = np.random.randint(0, 2, (25, 1)).astype(np.float32)  # Labels each one either Red or Blue with numbers 0 and 1\n",
    "red = trainData[responses.ravel() == 0]  # Take Red families and plot them\n",
    "plt.scatter(red[:, 0], red[:, 1], 80, 'r', '^')\n",
    "blue = trainData[responses.ravel() == 1]  # Take Blue families and plot them\n",
    "plt.scatter(blue[:, 0], blue[:, 1], 80, 'b', 's')\n",
    "newcomer = np.random.randint(0, 100, (1, 2)).astype(np.float32)  # New comer is marked in green color\n",
    "plt.scatter(newcomer[:, 0], newcomer[:, 1], 80, 'g', 'o')\n",
    "knn = cv2.ml.KNearest_create()\n",
    "knn.train(trainData, cv2.ml.ROW_SAMPLE, responses)\n",
    "ret, results, neighbours, dist = knn.findNearest(newcomer, 7)\n",
    "print \"result: \", results\n",
    "print \"neighbours: \", neighbours\n",
    "print \"distance: \", dist\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVN(Support Vector Machines)支持向量机\n",
    "> [用于分类的一种算法](http://cuijiahua.com/blog/2017/11/ml_8_svm_1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K均值算法(K-Means)\n",
    "> K均值(K-Means)算法是一种无监督的聚类学习算法，它会尝试找到样本数据的自然类别；分类K由用户自定义，K均值在不需要任何其他先验知识的情况下，依据算法的迭代规则，把样本划分为K类  \n",
    "\n",
    "> K均值是最常用的聚类技术之一，通过不断迭代和移动质心来完成分类\n",
    "\n",
    "### K均值算法的实现过程：\n",
    "> 1. 对一组未知分类的数据集合，指定其分类数`K`\n",
    "> 2. 随机分配`K`个类别的中心点位置，分配的原则是各个类别的中心点距离彼此越远越好\n",
    "> 3. 将数据集中的每一个点进行类别划分，与`K`个初始类别中的哪一个中心点距离最近，就划入那一类\n",
    "> 4. 根据上一步初步划分的`K`个类别，分别计算每一类的样品中心，并移动初始中心点到当前集合所在的中心\n",
    "> 5. 去除数据集合中每个点的归类属性(就是每个点不再属于任何分类)，依据上一步产生的新中心点，转到第3步，迭代执行，直到中心点收敛\n",
    "\n",
    "> K均值的核心就是不断移动类别划分的中心点，直到该点稳定下来或者达到所设置的最大迭代次数，这时当前中心点所划分的类别就是最终的K均值对样本数据的聚类\n",
    "\n",
    "### K均值算法的缺陷：\n",
    "> 1. K值的选择需要用户指定，实际中K值的估计很难做到准确，并且不同的K值得到的结果可能差别很大\n",
    "> 2. 初始的聚类中心点的设定对结果影响较大。不同的初始聚类中心可能导致完全不同的聚类结果，并且不能保证K-Means算法收敛于全局最优解，极端情况下有可能达到局部收敛\n",
    "> 3. 时间复杂度高。数据库较大的时候，收敛会比较慢\n",
    "\n",
    "### 函数原型及示例\n",
    "> 函数原型\n",
    "```python\n",
    "kmeans(data, K, bestLabels, criteria, attempts, flags[, centers]) -> retval, bestLabels, centers\n",
    "```\n",
    "> 输入参数\n",
    "```python\n",
    "1. data: 待处理数据集，每个特征应该放一列\n",
    "2. K: 分类的最终数目\n",
    "3. criteria: 终止迭代的条件，是一个含有3个成员的元组(typw, max_iter, epsilon)\n",
    "   * typw: 指定终止的类型，有如下三种选择：\n",
    "      - cv2.TERM_CRITERIA_EPS 只有精确度 epsilon 满足是停止迭代\n",
    "      - cv2.TERM_CRITERIA_MAX_ITER 当迭代次数超过阈值时停止迭代\n",
    "      - cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER 上面的任何一个条件满足时停止迭代  \n",
    "   * max_iter: 表示最大迭代次数  \n",
    "   * epsilon:  精确度阈值\n",
    "4. attempts: 使用不同的起始标记来执行算法的次数\n",
    "5. flags: 用来设置如何选择起始重心。通常有两个选择：\n",
    "   - cv2.KMEANS_PP_CENTERS  \n",
    "   - cv2.KMEANS_RANDOM_CENTERS  \n",
    "```\n",
    "> 输出参数\n",
    "```python\n",
    "1. retval: 紧密度，返回每个点到相应重心的距离的平方和\n",
    "2. bestLabels: 标志数组，每个成员被标记为 0, 1 等\n",
    "3. centers: 由聚类的中心组成的数组\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 视频分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Meanshift](https://blog.csdn.net/xiao__run/article/details/77135375)\n",
    "> Meanshift 算法用于在视频中找到并跟踪目标对象。meanshift算法思想其实很简单：利用概率密度的梯度爬升来寻找局部最优。它要做的就是输入一个范围(在图像内)，然后一直迭代(朝着重心迭代)直到满足要求为止。\n",
    "\n",
    "> 要在 OpenCV 中使用 Meanshift 算法首先要对目标对象进行设置，计算目标对象的直方图，这样在执行meanshift 算法时就可以将目标对象反向投影到每一帧中去了；另外还需要提供窗口的起始位置。\n",
    "\n",
    "> 函数原型:\n",
    "```python\n",
    "cv2.meanShift(probImage, window, criteria)\n",
    "```\n",
    ">> 输入一张图像(probImage)，再输入一个开始迭代的方框(window)和一个迭代条件(criteria)，输出的是迭代完成的位置（comp ）\n",
    "\n",
    "> meanshift算法优势：\n",
    ">> 1. 算法计算量不大，在目标区域已知的情况下完全可以做到实时跟踪\n",
    ">> 2. 采用核函数直方图模型，对边缘遮挡、目标旋转、变形和背景运动不敏感\n",
    "\n",
    "> meanshift算法缺点:\n",
    ">> 1. 缺乏必要的模板更新；\n",
    ">> 2. 跟踪过程中由于窗口宽度大小保持不变，当目标尺度有所变化时，跟踪就会失败；\n",
    ">> 3. 当目标速度较快时，跟踪效果不好；\n",
    ">> 4. 直方图特征在目标颜色特征描述方面略显匮乏，缺少空间信息；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camshift\n",
    "> 算法首先要使用 meanshift 找到(并覆盖)目标之后，再去调整窗口的大小；它还会计算目标对象的最佳外接椭圆的\n",
    "角度，并以此调节窗口角度。然后使用更新后的窗口大小和角度来在原来的位置继续进行 meanshift。重复这个过程直到达到需要的精度。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
