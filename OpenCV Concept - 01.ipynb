{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#尺度空间极值检测\" data-toc-modified-id=\"尺度空间极值检测-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>尺度空间极值检测</a></span><ul class=\"toc-item\"><li><span><a href=\"#背景\" data-toc-modified-id=\"背景-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>背景</a></span></li><li><span><a href=\"#原理\" data-toc-modified-id=\"原理-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>原理</a></span></li></ul></li><li><span><a href=\"#非极大值抑制(Non-maximum-suppression)\" data-toc-modified-id=\"非极大值抑制(Non-maximum-suppression)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>非极大值抑制(Non-maximum suppression)</a></span><ul class=\"toc-item\"><li><span><a href=\"#实现步骤\" data-toc-modified-id=\"实现步骤-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>实现步骤</a></span></li><li><span><a href=\"#应用范围\" data-toc-modified-id=\"应用范围-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>应用范围</a></span></li></ul></li><li><span><a href=\"#汉明距离(Hamming-Distance)\" data-toc-modified-id=\"汉明距离(Hamming-Distance)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span><a href=\"https://blog.csdn.net/chouisbo/article/details/54906909\" target=\"_blank\">汉明距离(Hamming Distance)</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#汉明距离的意义\" data-toc-modified-id=\"汉明距离的意义-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>汉明距离的意义</a></span></li><li><span><a href=\"#汉明距离的计算\" data-toc-modified-id=\"汉明距离的计算-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>汉明距离的计算</a></span></li></ul></li><li><span><a href=\"#明氏距离\" data-toc-modified-id=\"明氏距离-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>明氏距离</a></span></li><li><span><a href=\"#欧式距离\" data-toc-modified-id=\"欧式距离-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>欧式距离</a></span></li><li><span><a href=\"#曼哈顿距离\" data-toc-modified-id=\"曼哈顿距离-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>曼哈顿距离</a></span></li><li><span><a href=\"#归一化\" data-toc-modified-id=\"归一化-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>归一化</a></span></li><li><span><a href=\"#方向梯度直方图HOG-(Histogram-of-Oriented-Gradients)\" data-toc-modified-id=\"方向梯度直方图HOG-(Histogram-of-Oriented-Gradients)-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>方向梯度直方图HOG (Histogram of Oriented Gradients)</a></span></li><li><span><a href=\"#BOW(Bag-of-Word)-词袋\" data-toc-modified-id=\"BOW(Bag-of-Word)-词袋-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>BOW(Bag of Word) 词袋</a></span><ul class=\"toc-item\"><li><span><a href=\"#示例讲解BOW\" data-toc-modified-id=\"示例讲解BOW-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span><a href=\"https://blog.csdn.net/Real_Myth/article/details/51239782?locationNum=15\" target=\"_blank\">示例讲解BOW</a></a></span></li><li><span><a href=\"#BOW实现步骤\" data-toc-modified-id=\"BOW实现步骤-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span><a href=\"https://blog.csdn.net/xuzhongxiong/article/details/52689048\" target=\"_blank\">BOW实现步骤</a></a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 尺度空间极值检测\n",
    "\n",
    "### 背景\n",
    "> Harris算法具有旋转不变性，即使图片发生了旋转，也能找到同样的角点；但是，如果对图像进行缩放后，角点可能就不再是角点了；例如，在一副小图中使用一个小的窗口可以检测到一个角点，但是如果图像被放大，再使用同样的窗口就检测不到角点了  \n",
    "\n",
    "### 原理\n",
    "> 不同的尺度空间不能使用相同的窗口检测极点值；对小的角点要用小的窗口，对大的角点只能使用大的窗口，为了达到这个目的，需要使用尺度空间滤波器（尺度空间滤波器可以使用一些具有不同方差的高斯卷积核构成）  \n",
    "> 使用具有不同方差值的`高斯拉普拉斯算子`(LoG)对图像进行卷积，(LoG)由于具有不同的方差值所以可以用来检测不同大小的斑点（当(LoG)的方差与斑点直径相等时能够使斑点完全平滑）。简单来说方差就是一个尺度变换因子。  \n",
    "> 高斯方差的大小与窗口的大小存在一个倍数关系：窗口大小等于`6`倍方差加`1`，所以方差的大小也决定了窗口大小；使用一个小方差的高斯卷积核是可以很好的检测出小的角点，而使用大方差的高斯卷积核时可以很好的检测除大的角点 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非极大值抑制(Non-maximum suppression)\n",
    "> 非极大值抑制就是抑制不是极大值的元素，搜索局部的极大值  \n",
    "> 非最大(或非极大)抑制是一种与`图像同一区域相关的所有结果进行抑制的技术`，从一系列`围绕同一主题的重叠窗口`中保留最佳窗口，丢弃评分较低的重叠窗口(因此，大于指定阀值的所有窗口通常都要进行非最大抑制操作)\n",
    "\n",
    "### 实现步骤\n",
    "> 1. 一旦建立了图像金字塔，为了检测目标，可采用滑动窗口来搜索图像\n",
    "> 2. 收集当前所有含有目标的窗口(超出一定任意阀值)，并得到最高相应的窗口`W`\n",
    "> 3. 消除所有与`W`有明显重叠的窗口\n",
    "> 4. 移动到下一个有最高响应的窗口，在当前尺度下重复上述过程\n",
    "> 5. 在这个过程完成后，移动到图像金字塔的下一个尺度，并重复前面的过程\n",
    "> 6. 上述过程结束后，会得到一系列评分最高的窗口(可以检查完全包含在其他窗口中的窗口，并消除这些窗口)\n",
    "\n",
    "<img src=\"illustratin/nms.png\">\n",
    "\n",
    "### 应用范围\n",
    "> 非极大值抑制NMS在目标检测，定位等领域是一种被广泛使用的方法。对于目标具体位置定位过程，不管是使用sw(sliding Window)还是ss(selective search)方法，都会产生好多的候选区域。实际看到的情形就是好多区域的交叉重叠，难以满足实际的应用  \n",
    "\n",
    "> 针对该问题的有３种传统的解决思想：\n",
    ">> * 第一种，选取好多矩形框的交集，即公共区域作为最后的目标区域。\n",
    "\n",
    ">> * 第二种，选取好多矩形框的并集，即所有矩形框的最小外截矩作为目标区域。当然这里也不是只要相交就直接取并集，需要相交的框满足交集占最小框的面积达到一定比例(也就是阈值)才合并。\n",
    "\n",
    ">> * 第三种,也就是本文的NMS，简单的说，对于有相交的就选取其中置信度最高的一个作为最后结果，对于没相交的就直接保留下来，作为最后结果。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [汉明距离(Hamming Distance)](https://blog.csdn.net/chouisbo/article/details/54906909)\n",
    "> 汉明距离表示两个等长字符串在对应位置上不同字符的数目，我们以d(x, y)表示字符串x和y之间的汉明距离。从另外一个方面看，汉明距离度量了通过替换字符的方式将字符串x变成y所需要的最小的替换次数。\n",
    "\n",
    "> 举例说明以下字符串间的汉明距离为：\n",
    "```python\n",
    "\"karolin\" and \"kathrin\" is 3.\n",
    "\"karolin\" and \"kerstin\" is 3.\n",
    "1011101 and 1001001 is 2.\n",
    "2173896 and 2233796 is 3.\n",
    "```\n",
    "\n",
    "### 汉明距离的意义\n",
    "> 对于二进制串ａ和ｂ来说，汉明距离等于ａ`XOR`ｂ中１的数目，我们又称其为汉明权重，也叫做population count或popcount。长度为ｎ的二进制字符串通过汉明距离构成了一个度量空间(metric space)，我们称其为汉明立方(Hamming Cube)。\n",
    "\n",
    "### 汉明距离的计算\n",
    "> python中简单计算汉明距离的代码如下\n",
    "```python\n",
    "def hammingDistance(s1, s2):\n",
    "    '''\n",
    "       Return the Hamming distance between equal-length sequences\n",
    "    '''\n",
    "    if len(s1) != len(s2):\n",
    "        raise ValueError(\"Undefined for unequal length\")\n",
    "    return sum(el1 != el2 for el1, el2 in zip(s1, s2))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 明氏距离\n",
    "> 明氏距离又叫做明可夫斯基距离（Minkowski distance），根本不是种概念，或者可以说是以一种集合或者公式。\n",
    ">> * 当纬度等于1时候，其公式等价于曼哈顿距离。\n",
    ">> * 当纬度等于2时候，其公式等价于欧式距离。\n",
    ">> * 当纬度当大于2到无穷大时候，其公式等价于切比雪夫距离。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 欧式距离"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 曼哈顿距离"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 归一化\n",
    "> 归一化就是要把需要处理的数据经过处理后（通过某种算法）限制在所需要的一定范围内。首先归一化是为了后面数据处理的方便，其次是保证程序运行时收敛加快。归一化的具体作用是归纳统一样本的统计分布性。归一化在0-1之间是统计的概率分布，归一化在某个区间上是统计的坐标分布。归一化有同一、统一和合一的意思\n",
    "\n",
    "> 归一化的目的简而言之，是使得没有可比性的数据变得具有可比性，同时又保持相比较的两个数据之间的相对关系，如大小关系；或是为了作图，原来很难在一张图上作出来，归一化后就可以很方便的给出图上的相对位置等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方向梯度直方图HOG (Histogram of Oriented Gradients)\n",
    "> 在一幅图像中，梯度或边缘的方向密度分布能够很好地描述局部目标区域的特征，HOG正是利用这种思想，将图像局部出现的方向梯度次数进行计数，该方法和边缘方向直方图、SIFT类似，不同的是hog的计算基于一致空间的密度矩阵来提高准确率。\n",
    "\n",
    "> 在`HOG`中，对一幅图像进行了如下划分：   \n",
    ">> 图像(image) -> 检测窗口(win) -> 图像块(block) -> 细胞单元(cell)\n",
    "\n",
    "> HOG特征原理:  \n",
    "HOG的核心思想是所检测的局部物体外形能够被光强梯度或边缘方向的分布所描述。通过将整幅图像分割成小的连接区域(称为cells)，每个cell生成一个方向梯度直方图或者cell中pixel的边缘方向，这些直方图的组合可表示出(所检测目标的目标)描述子。为改善准确率，局部直方图可以通过计算图像中一个较大区域(称为block)的光强作为measure被对比标准化，然后用这个值(measure)归一化这个block中的所有cells.这个归一化过程完成了更好的照射/阴影不变性。   \n",
    "与其他描述子相比，HOG得到的描述子保持了几何和光学转化不变性（除非物体方向改变）。因此HOG描述子尤其适合人的检测。\n",
    "\n",
    "> [HOG特征提取算法的实现过程：](https://blog.csdn.net/ppp8300885/article/details/71078555)\n",
    ">> 1. 读入所需要的检测目标(即输入image)\n",
    ">> 2. 将图像进行灰度化\n",
    ">> 3. 采用Gamma校正法对输入图像进行颜色空间的标准化(归一化)\n",
    ">> 4. 计算图像每个像素的梯度(包括大小和方向)，捕获轮廓信息\n",
    ">> 5. 统计每个cell的梯度直方图(不同梯度的个数)，形成每个cell的descriptor\n",
    ">> 6. 将每几个cell组成一个block(以３＊３为例)，一个block内所有cell的特征串联起来得到该block的HOG特征descriptor\n",
    ">> 7. 将图像image内所有block的HOG特征descriptor串联起来得到该image的HOG特征descriptor，这就是最终分类的特征向量  \n",
    "\n",
    "<img src=\"illustratin/hog.png\">    \n",
    ">> 对于上述流程图，有几点需要注意的地方： \n",
    "* 色彩和伽马归一化为了减少光照因素的影响，首先需要将整个图像进行规范化（归一化）。在图像的纹理强度中，局部的表层曝光贡献的比重较大，所以，这种压缩处理能够有效地降低图像局部的阴影和光照变化     \n",
    "* 图像的梯度针对的是每一个像素计算得到，然后再cell中进行方向梯度直方图的构建，在block中进行对比度归一化操作  \n",
    "* 由于窗口的滑动性与块的滑动行，窗口与块都会出现不同程度的重叠（由步长决定），此时在块内划分出的cell就会多次出现，这就意味着：每一个细胞单元的输出都多次作用于最终的描述器  \n",
    "\n",
    "> [HOG、SIFT与PCA-SIFT的应用与区别](https://blog.csdn.net/yangtrees/article/details/7463431)\n",
    "> 1. SIFT由于其庞大计算量不用与行人检测\n",
    "> 2. PCA-SIFT的方法过滤掉很多维度的信息，只保留20个主分量，因此只适用于行为变化不大的物体检测\n",
    "> 3. HOG是一个基于梯度的直方图提取算法，用于人体检测十分有效\n",
    "\n",
    "> * Hog没有旋转和尺度不变性，因此计算量小；而SIFT中每个feature需要用128维的向量来描述，因此计算量相对很大。   \n",
    "> * 为了解决`Scale-invariant`的问题：将图片进行不同尺度的缩放，就相当于对模板进行不同尺度scale的缩放  \n",
    "> * 为了解决`Rotation-invariant`的问题：建立不同方向的模版(一般取15x7的)进行匹配  \n",
    "> * 总的来说，就是在不同尺度上的图像进行不同方向的模板(15x7)匹配，每个点形成一个8方向的梯度描述  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW(Bag of Word) 词袋\n",
    "> Bag of word 模型最初被用在文本分类中， 将文档表示成特征矢量。它的基本思想是假定对于一个文本，忽略其词序和语法、句法，仅仅将其看做是一些词汇的集合，而文本的每个词汇都是独立的。  \n",
    "简单说就是将每个文档看成一个袋子，然后看每个袋子里装的都是些什么词汇，将其分类。通过统计文档中各个词出现的次数判断该文档在描述什么。例如，一个分类中如果猪、牛、马、羊、山谷、土地、拖拉机这样的词汇多，而银行、大厦、汽车、公园这样的词汇少，就可以断定该文档描述的乡村的场景，而不是城镇。   \n",
    "\n",
    "> BOW也可以应用在视觉领域的图像分类中：将图像类比成文档，即一幅图像类比成一个文档，将图像中提取的诸如SIFT特征点类比成文档中的单词，然后把从图像库中所有提取的所有SIFT特征点弄在一块进行聚类，从中得到具有代表性的聚类中心(单词)，再对每一幅图像中的SIFT特征点找距离它最近的聚类中心(单词)，做词频(TF)统计    \n",
    "\n",
    "### [示例讲解BOW](https://blog.csdn.net/Real_Myth/article/details/51239782?locationNum=15)\n",
    "> 美国总统全国大选为例，假设有10000个比较有影响力的人参加总统竞选，这10000个人表示的就是聚类中心，他们最具有代表性(K-means做的就是得到那些设定数目的最具有代表性的特征点)，每个州类比成一幅图像，州里的人手里持的票就好比是SIFT特征点，这样的话，我们就可以对每个州做一个10000维的票数统计结果，这个统计出来的就是`词频(TF)向量`。另外，我们还可以统计每个竞选人有多少个州投了他的票，那么就可以得到一个10000维长的对州的统计结果，这个结果再稍微和对数做下处理，便得到了所谓的`逆文档(IDF)词频`。\n",
    "\n",
    "### [BOW实现步骤](https://blog.csdn.net/xuzhongxiong/article/details/52689048)\n",
    "> 1. 创建词汇（特征）词典\n",
    ">> 以SIFT 128维特征作为例子。例如现在有1000张训练图片，对每一张训练图片都提取SIFT的128维特征，那么最终可以得到N(i)x128的特征，N(i)代表第i张图特征点的个数，因为每张图像不一样，所以每张图像的SIFT 特征个数也不一样。提取特征后对1000张图像提取出的所有SIFT特征进行聚类（目的是为了合并那些相近的特征，相当于集合的合并操作），常用的聚类方法是k-均值聚类。对以上例子对∑N(i)个特征选择1000聚类中心进行聚类，将这1000个聚类中心称为词典，这个词典好比一个容器，通俗一点就是一个直方图的基，利用这个基去统计这些样本的信息。\n",
    "> 2. 得到训练数据到字典的映射\n",
    ">> 通过相关的映射，得到不同类别的一个类别的直方图统计 <img src=\"illustratin/bow_1.jpg\"> \n",
    "> 3. 选择适当的分类器进行训练\n",
    ">> 分类器可以选择朴素贝叶斯分类器或SVM。有研究表明BOW结合SVM效果比较好。训练的时候，将步骤2中得到的相关系数作为输入，送入到SVM分类器中进行训练\n",
    "> 4. 对新来的样本，先映射到字典空间，然后利用得到的分类器进行分类\n",
    ">> 训练好分类器后，对于新来的样本，同样先提取SIFT特征，然后将SIFT特征映射到词典中去，然后得到的直方图就可以通过分类器进行分类了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "272px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
