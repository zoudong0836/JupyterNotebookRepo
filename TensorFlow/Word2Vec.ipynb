{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#NLP词的表示方法\" data-toc-modified-id=\"NLP词的表示方法-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>NLP词的表示方法</a></span><ul class=\"toc-item\"><li><span><a href=\"#词的独热表示-(one-hot-representation)\" data-toc-modified-id=\"词的独热表示-(one-hot-representation)-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>词的独热表示 (one-hot representation)</a></span></li><li><span><a href=\"#词的分布式表示-(distributed-representation)\" data-toc-modified-id=\"词的分布式表示-(distributed-representation)-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>词的分布式表示 (distributed representation)</a></span></li></ul></li><li><span><a href=\"#基于神经网络的分布式表示\" data-toc-modified-id=\"基于神经网络的分布式表示-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>基于神经网络的分布式表示</a></span><ul class=\"toc-item\"><li><span><a href=\"#CBOW-(Continous-Bag-of-Word-Model)\" data-toc-modified-id=\"CBOW-(Continous-Bag-of-Word-Model)-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>CBOW (Continous Bag-of-Word Model)</a></span></li><li><span><a href=\"#Skip-gram\" data-toc-modified-id=\"Skip-gram-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Skip-gram</a></span></li></ul></li><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Word2Vec</a></span><ul class=\"toc-item\"><li><span><a href=\"#基于Hierarchical-Softmax的模型\" data-toc-modified-id=\"基于Hierarchical-Softmax的模型-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>基于Hierarchical Softmax的模型</a></span></li><li><span><a href=\"#基于Negative-Sampling的模型-(负采样)\" data-toc-modified-id=\"基于Negative-Sampling的模型-(负采样)-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>基于Negative Sampling的模型 (负采样)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP词的表示方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词的独热表示 (one-hot representation)\n",
    "词向量维度大小为整个词汇表的大小，对于每个具体的词汇表中的词，将对应的位置置为1。比如有5个词组成的词汇表，词”Queen”在词汇表中的序号为2， 那么它的词向量就是(0,1,0,0,0)。同样的道理，词”Woman”是序号3，词向量就是(0,0,0,1,0)。这种词向量的编码方式一般叫做one hot representation.\n",
    "\n",
    "<br/>one-hot representation用来表示词向量非常简单，但是却有很多问题：\n",
    "1.  任意两个词之间都是孤立的，根本无法表示出在语义层面上词语词之间的相关信息，而这一点是致命的。\n",
    "2. 词汇表一般都非常大，比如达到百万级别，这样每个词都用百万维的向量来表示简直是内存的灾难"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词的分布式表示 (distributed representation)\n",
    "Dristributed representation可以解决one-hot representation的问题，它的思路是通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。这个较短的词向量维度是多大呢？这个一般需要在训练时自己来指定。   \n",
    "\n",
    "<br/>词的分布式表示主要可以分为三类：\n",
    "1.  基于矩阵的分布表示\n",
    "2. 基于聚类的分布表示\n",
    "3. 基于神经网络的分布表示\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于神经网络的分布式表示\n",
    "目前主要使用的两种是：\n",
    "* CBOW (Continuous Bag of Words)\n",
    "> 从原始语句推测目标字词，对小型数据比较适合\n",
    "* Skip-Gram\n",
    "> 从目标字词推测出原始语句，在大型语料中表现更好\n",
    "\n",
    "无论是CBOW还是Skip-gram模型，其输出我们不关心，我们更关心的是模型中第一个隐含层中的参数权重，这个参数矩阵就是我们需要的词向量。它的每一行就是词典中对应词的词向量，行数就是词典的大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW (Continous Bag-of-Word Model)\n",
    "又称连续词袋模型，是一个三层神经网络；该模型的特点是输入已知上下文，输出对当前单词的预测。  \n",
    "> 假设上下文对应的词有8个，前后各4个，这8个词是模型的输入。由于CBOW使用的是词袋模型，因此这8个词都是平等的，也就是不考虑它们和关注的词之间的距离大小，只要在上下文之内即可  \n",
    "\n",
    "> 输入是8个词向量，输出是所有词的softmax概率 (训练的目标是期望训练样本特定词对应的softmax概率最大)，对应的CBOW神经网络模型输入层有8个神经元，输出层有词汇表大小个神经元\n",
    "\n",
    "![CBOW模型示意图](illustration/CBOW模型示意图.jpeg)\n",
    "\n",
    "> 输入的是 one-hot 向量，第一层是一个全连接层，然后没有激活函数，输出层是一个softmax层，输出一个概率分布，表示词典中每个词出现的概率。我们并不关心输出的内容，训练完成后第一个全连接层的参数就是`word embedding`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-gram \n",
    "只是逆转了CBOW的因果关系，即已知当前词语，预测上下文\n",
    "\n",
    "> 假设上下文大小取值为4，则模型的输出是8个上下文词，前后各4个；在Skip-Gram模型中，输入特定词，输出是softmax概率排前8的8个词\n",
    "\n",
    "> 对应的Skip-Gram神经网络模型输入层有1个神经元，输出层有词汇表大小个神经元。隐藏层的神经元个数可以自己指定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec \n",
    "也称为`Word Embeddings`，中文`词向量`或`词嵌入`。Word2Vec是google在2013年推出的一个NLP(自然语言处理)工具，其特点是将所有的词向量化，这样词与词之间就可以定量的去度量他们之间的关系，挖掘词之间的联系。\n",
    "\n",
    "> Word2vec主要分为两种模式\n",
    "* CBOW (Continuous Bag of Words)\n",
    "* Skip-Gram\n",
    "\n",
    "> Word2vec虽然使用了CBOW与Skip-Gram来训练模型与得到词向量，但是并没有使用传统的NN模型。最先优化使用的数据结构是用霍夫曼树来代替隐藏层和输出层的神经元，霍夫曼树的叶子节点起到输出层神经元的作用，叶子节点的个数即为词汇表的小大。 而内部节点则起到隐藏层神经元的作用。Word2vec有两种改进方法，\n",
    "* 一种是基于Hierarchical Softmax\n",
    "* 一种是基于Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于Hierarchical Softmax的模型\n",
    "\n",
    "传统的神经网络词向量语言模型，里面一般有三层，输入层（词向量），隐藏层和输出层（softmax层）。里面最大的问题在于从隐藏层到输出的softmax层的计算量很大，因为要计算所有词的softmax概率，再去找概率最大的值。word2vec对这个模型做了改进：\n",
    "1.  第一个改进是对于从输入层到隐藏层的映射，没有采取神经网络的线性变换加激活函数的方法，而是采用简单的对所有输入词向量`求和并取平均`的方法。比如输入的是三个4维词向量：(1,2,3,4), (9,6,11,8), (5,10,7,12),那么我们word2vec映射后的词向量就是(5,6,7,8)。由于这里是从多个词向量变成了一个词向量\n",
    "2.  第二个改进是从隐藏层到输出的softmax层这里的计算量的改进。为了避免要计算所有词的softmax概率，word2vec采用了霍夫曼树来代替从隐藏层到输出softmax层的映射 (霍夫曼树的所有内部节点就类似之前神经网络隐藏层的神经元，其中，根节点的词向量对应投影后的词向量，而所有叶子节点就类似于之前神经网络softmax输出层的神经元，叶子节点的个数就是词汇表的大小)\n",
    "\n",
    "<br/>Hierarchical Softmax的的缺点\n",
    "> 使用霍夫曼树来代替传统的神经网络，可以提高模型训练的效率。但是如果训练样本里的中心词是一个很生僻的词，那么就得在霍夫曼树中辛苦的向下走很久"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于Negative Sampling的模型 (负采样)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
