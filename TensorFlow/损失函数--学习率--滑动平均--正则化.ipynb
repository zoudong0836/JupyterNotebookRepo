{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#NN-(Neural-Network)-神经网络\" data-toc-modified-id=\"NN-(Neural-Network)-神经网络-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>NN (Neural Network) 神经网络</a></span></li><li><span><a href=\"#损失函数(loss)\" data-toc-modified-id=\"损失函数(loss)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>损失函数(loss)</a></span></li><li><span><a href=\"#学习率(learning_rate)\" data-toc-modified-id=\"学习率(learning_rate)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>学习率(learning_rate)</a></span></li><li><span><a href=\"#滑动平均(影子值)\" data-toc-modified-id=\"滑动平均(影子值)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>滑动平均(影子值)</a></span></li><li><span><a href=\"#正则化\" data-toc-modified-id=\"正则化-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>正则化</a></span></li><li><span><a href=\"#神经网络搭建套路\" data-toc-modified-id=\"神经网络搭建套路-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>神经网络搭建套路</a></span></li><li><span><a href=\"#手写数字识别示例(MNIST)\" data-toc-modified-id=\"手写数字识别示例(MNIST)-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>手写数字识别示例(MNIST)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN (Neural Network) 神经网络\n",
    "> NN复杂度： 常用NN层数和NN参数的个数表示\n",
    ">> * 层数 = 隐藏层的层数 + 1个输出层 (输入层不纳入计算)\n",
    ">> * 总参数 = 总W + 总b  \n",
    ">>> 每一层的 `W` 个数等于 `上一层神经元个数` x `本层神经元个数`    \n",
    ">>> 每一层的 `b` 个数等于 `本层神经元个数`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数(loss)\n",
    "[Link1](https://blog.csdn.net/marsjhao/article/details/72630147)\n",
    "\n",
    "> 预测值(y)与已知答案(y_)的差距  \n",
    "\n",
    "> NN优化的目标: loss最小化\n",
    ">> 1. mse (Mean Squared Error) 均方误差\n",
    ">>> loss_mse = tf.reduce_mean(tf.square(y_ - y))\n",
    ">> 2. 自定义\n",
    ">>> loss = tf.reduce_sum(tf.where(tf.greater(y, y_), arg1*(y - y_), arg2*(y_ - y)))\n",
    ">> 3. ce (Cross Entropy) 交叉熵 -- `表征两个概率分布之间的距离`\n",
    ">>> ce = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-12, 1.0)))   \n",
    ">>>> tf.clip_by_value()函数可将一个tensor的元素数值限制在指定范围内，这样可防止一些错误运算，起到数值检查作用\n",
    "\n",
    "> TensorFlow 提供了集成交叉熵函数  \n",
    ">>> * ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits_v2(logits=y, labels=y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "\n",
    "\"\"\"\n",
    "  '均方误差' 损失函数\n",
    "\"\"\"\n",
    "\n",
    "#0导入模块，生成数据集\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "BATCH_SIZE = 8\n",
    "SEED = 23455\n",
    "\n",
    "rdm = np.random.RandomState(SEED)\n",
    "X = rdm.rand(32,2)\n",
    "Y_ = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1, x2) in X]\n",
    "\n",
    "#1定义神经网络的输入、参数和输出，定义前向传播过程。\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "w1= tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1))\n",
    "y = tf.matmul(x, w1)\n",
    "\n",
    "#2定义损失函数及反向传播方法。\n",
    "#定义损失函数为MSE,反向传播方法为梯度下降。\n",
    "loss_mse = tf.reduce_mean(tf.square(y_ - y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss_mse)\n",
    "\n",
    "#3生成会话，训练STEPS轮\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 20000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = (i*BATCH_SIZE) % 32 + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y_[start:end]})\n",
    "        if i % 500 == 0:\n",
    "            print \"After %d training steps, w1 is: \" % (i)\n",
    "            print sess.run(w1), \"\\n\"\n",
    "    print \"Final w1 is: \\n\", sess.run(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "\n",
    "\"\"\"\n",
    "  '自定义' 损失函数\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "BATCH_SIZE = 8\n",
    "SEED = 23455\n",
    "COST = 1     # 酸奶成本1元\n",
    "PROFIT = 9   # 酸奶利润9元\n",
    "#预测少了损失大，故不要预测少，故生成的模型会多预测一些\n",
    "\n",
    "rdm = np.random.RandomState(SEED)\n",
    "X = rdm.rand(32,2)\n",
    "Y = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1, x2) in X]\n",
    "\n",
    "#1定义神经网络的输入、参数和输出，定义前向传播过程。\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "w1= tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1))\n",
    "y = tf.matmul(x, w1)\n",
    "\n",
    "#2定义损失函数及反向传播方法。\n",
    "# 定义损失函数使得预测少了的损失大，于是模型应该偏向多的方向预测。\n",
    "loss = tf.reduce_sum(tf.where(tf.greater(y, y_), (y - y_)*COST, (y_ - y)*PROFIT))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "\n",
    "#3生成会话，训练STEPS轮\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 3000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = (i*BATCH_SIZE) % 32 + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "        if i % 500 == 0:\n",
    "            print \"After %d training steps, w1 is: \" % (i)\n",
    "            print sess.run(w1), \"\\n\"\n",
    "    print \"Final w1 is: \\n\", sess.run(w1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "\n",
    "\"\"\"\n",
    "  '交叉熵' 损失函数\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "BATCH_SIZE = 8\n",
    "SEED = 23455\n",
    "\n",
    "rdm = np.random.RandomState(SEED)\n",
    "X = rdm.rand(32,2)\n",
    "Y = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1, x2) in X]\n",
    "\n",
    "#1定义神经网络的输入、参数和输出，定义前向传播过程。\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "w1= tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1))\n",
    "y = tf.matmul(x, w1)\n",
    "\n",
    "#2定义损失函数及反向传播方法。\n",
    "#重新定义损失函数，使得预测多了的损失大，于是模型应该偏向少的方向预测。\n",
    "'''\n",
    "** https://blog.csdn.net/m0_37041325/article/details/77043598\n",
    "1. 这个函数和tf.nn.softmax_cross_entropy_with_logits函数比较明显的区别在于它的参数labels的不同，\n",
    "这里的参数label是非稀疏表示的\n",
    "2. 稀疏表示的形式为[0,0,1](one-hot vector), 这个表示这个样本为第3个分类;\n",
    "而非稀疏表示就表示为2(scala)（因为从0开始算，0,1,2,就能表示三类）\n",
    "3. tf.nn.sparse_softmax_cross_entropy_with_logits函数\n",
    "比tf.nn.softmax_cross_entropy_with_logits多了一步操作，将labels稀疏化的操作\n",
    "'''\n",
    "ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "loss = tf.reduce_mean(ce)\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "\n",
    "#3生成会话，训练STEPS轮\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 10000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = (i*BATCH_SIZE) % 32 + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "        if i % 500 == 0:\n",
    "            print \"After %d training steps, w1 is: \" % (i)\n",
    "            print sess.run(w1), \"\\n\"\n",
    "    print \"Final w1 is: \\n\", sess.run(w1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "  'sparse_softmax_cross_entropy_with_logits' 函数示例\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "y2 = tf.convert_to_tensor([[0, 0, 1, 0]], dtype=tf.int64)\n",
    "y_2 = tf.convert_to_tensor([[-2.6, -1.7, 3.2, 0.1]], dtype=tf.float32)\n",
    "y_2_2 = tf.argmax(y2, 1)\n",
    "c2 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_2, labels=y_2_2)\n",
    "\n",
    "y3 = tf.convert_to_tensor([[0, 0, 1, 0], [0, 0, 1, 0]], dtype=tf.int64)\n",
    "y_3 = tf.convert_to_tensor([[-2.6, -1.7, -3.2, 0.1], [-2.6, -1.7, 3.2, 0.1]], dtype=tf.float32)\n",
    "y_3_3 = tf.argmax(y_3, 1)\n",
    "c3 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_3, labels=y_3_3)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    y2_ = sess.run(y2)\n",
    "    print y2_.shape, y2_        # (1, 4) [[0 0 1 0]]\n",
    "\n",
    "    y_2_ = sess.run(y_2)\n",
    "    print y_2_.shape, y_2_      # (1, 4) [[-2.6 -1.7  3.2  0.1]]\n",
    "\n",
    "    y_2_2_ = sess.run(y_2_2)\n",
    "    print y_2_2_.shape, y_2_2_  # (1,) [2]\n",
    "\n",
    "    c2_ = sess.run(c2)\n",
    "    print c2_.shape, c2_        # (1,) [0.05403664]\n",
    "\n",
    "    y3_ = sess.run(y3)\n",
    "    print y3_.shape, y3_\n",
    "\n",
    "    y_3_ = sess.run(y_3)\n",
    "    print y_3_.shape, y_3_\n",
    "\n",
    "    y_3_3 = sess.run(y_3_3)\n",
    "    print y_3_3.shape, y_3_3\n",
    "\n",
    "    c3_ = sess.run(c3)\n",
    "    print c3_.shape, c3_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习率(learning_rate)\n",
    "[Link1](https://www.imooc.com/article/details/id/27808)\n",
    "\n",
    "> 在神经网络训练过程中，参数每次更新的幅度\n",
    "\n",
    "> 学习率设置大了会震荡不收敛，学习率设置小了收敛速度慢  \n",
    ">> tf.train.GradientDescentOptimizer(learning_rate)  \n",
    "\n",
    "> 指数衰减计算公式\n",
    ">> decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n",
    ">>> * decayed_learning_rate: 每一轮优化时使用的学习率\n",
    ">>> * learning_rate: 超参数，事先设定(预估)的初始学习率\n",
    ">>> * decay_rate: 超参数，衰减系数\n",
    ">>> * decay_steps: 衰减速度 (即迭代多少次进行衰减)\n",
    ">>> * 一般来说，初始学习率、衰减系数和衰减速度都是根据经验设置的\n",
    "\n",
    "> 指数衰减学习率\n",
    ">> * global_step = tf.Variable(0, trainable=False)\n",
    ">>> 迭代次数初始值为0\n",
    ">> * learning_rate = tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate, staircase)\n",
    ">>> * 参数decay_steps=100时，即表示100轮迭代后进行一次衰减\n",
    ">>> * 参数staircase=True时，global_step/decay_steps会被转化为整数，这使得学习率呈阶梯型下降；若为False时，则是连续型下降\n",
    ">> * tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    ">>> 使用指数衰减的学习率，在minimize函数中传入global_step，它将自动更新，learning_rate也随即被更新\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "#设损失函数 loss=(w+1)^2, 令w初值是常数10。反向传播就是求最优w，即求最小loss对应的w值\n",
    "#使用指数衰减的学习率，在迭代初期得到较高的下降速度，可以在较小的训练轮数下取得更有收敛度。\n",
    "import tensorflow as tf\n",
    "\n",
    "LEARNING_RATE_BASE = 0.1   #最初学习率\n",
    "LEARNING_RATE_DECAY = 0.99 #学习率衰减率\n",
    "LEARNING_RATE_STEP = 1     #喂入多少轮BATCH_SIZE后，更新一次学习率，一般设为：总样本数/BATCH_SIZE\n",
    "\n",
    "#运行了几轮BATCH_SIZE的计数器，初值给0, 设为不被训练\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "#定义指数下降学习率\n",
    "learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, \n",
    "                                           global_step, \n",
    "                                           LEARNING_RATE_STEP, \n",
    "                                           LEARNING_RATE_DECAY, \n",
    "                                           staircase=True)\n",
    "#定义待优化参数，初值给10\n",
    "w = tf.Variable(tf.constant(5, dtype=tf.float32))\n",
    "#定义损失函数loss\n",
    "loss = tf.square(w+1)\n",
    "#定义反向传播方法\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate)\\\n",
    "                                            .minimize(loss, global_step=global_step)\n",
    "#生成会话，训练40轮\n",
    "with tf.Session() as sess:\n",
    "    init_op=tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for i in range(40):\n",
    "        sess.run(train_step)\n",
    "        learning_rate_val = sess.run(learning_rate)\n",
    "        global_step_val = sess.run(global_step)\n",
    "        w_val = sess.run(w)\n",
    "        loss_val = sess.run(loss)\n",
    "        print \"After %s steps: global_step is %f, w is %f, learning rate is %f, loss is %f\" % (i, global_step_val, w_val, learning_rate_val, loss_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 滑动平均(影子值)\n",
    "[Link1](https://blog.csdn.net/lanchunhui/article/details/70803060)\n",
    "\n",
    "> 记录了每个参数一段时间内过往值的平均，增加了模型的泛化性   \n",
    "> 滑动平均是针对所有参数：`W`和`b` \n",
    "\n",
    "> 滑动平均计算公式\n",
    ">> * 影子 = 衰减率 x 影子 + (1 - 衰减率) × 参数(W)     （影子初值 = 参数初值）   \n",
    ">> * 衰减率 = min{ MOVING_AVERAVG_DECAY, (1+轮数)/(10+轮数)}\n",
    "\n",
    "> 滑动平均函数\n",
    ">> * global_step = tf.Variable(0, trainable=False)\n",
    ">> * ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    ">> * ema_op = ema.apply(tf.trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import tensorflow as tf\n",
    "\n",
    "#1. 定义变量及滑动平均类\n",
    "#定义一个32位浮点变量，初始值为0.0  这个代码就是不断更新w1参数，优化w1参数，滑动平均做了个w1的影子\n",
    "w1 = tf.Variable(0, dtype=tf.float32)\n",
    "#定义num_updates（NN的迭代轮数）,初始值为0，不可被优化（训练），这个参数不训练\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "#实例化滑动平均类，给衰减率为0.99，当前轮数global_step\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "#ema.apply后的括号里是更新列表，每次运行sess.run（ema_op）时，对更新列表中的元素求滑动平均值。\n",
    "#在实际应用中会使用tf.trainable_variables()自动将所有待训练的参数汇总为列表\n",
    "#ema_op = ema.apply([w1])\n",
    "ema_op = ema.apply(tf.trainable_variables())\n",
    "\n",
    "#2. 查看不同迭代中变量取值的变化。\n",
    "with tf.Session() as sess:\n",
    "    # 初始化\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    #用 ema.average(w1)获取w1滑动平均值 （要运行多个节点，作为列表中的元素列出，写在sess.run中）\n",
    "    #打印出当前参数w1和w1滑动平均值\n",
    "    print \"current global_step:\", sess.run(global_step)\n",
    "    print \"current w1\", sess.run([w1, ema.average(w1)]) \n",
    "    \n",
    "    # 参数w1的值赋为1\n",
    "    sess.run(tf.assign(w1, 1))\n",
    "    sess.run(ema_op)\n",
    "    print \"current global_step:\", sess.run(global_step)\n",
    "    print \"current w1\", sess.run([w1, ema.average(w1)]) \n",
    "    \n",
    "    # 更新global_step和w1的值,模拟出轮数为100时，参数w1变为10, \n",
    "    # 以下代码global_step保持为100，每次执行滑动平均操作，影子值会更新 \n",
    "    sess.run(tf.assign(global_step, 100))  \n",
    "    sess.run(tf.assign(w1, 10))\n",
    "    sess.run(ema_op)\n",
    "    print \"current global_step:\", sess.run(global_step)\n",
    "    print \"current w1:\", sess.run([w1, ema.average(w1)])       \n",
    "    \n",
    "    # 每次sess.run会更新一次w1的滑动平均值\n",
    "    sess.run(ema_op)\n",
    "    print \"current global_step:\" , sess.run(global_step)\n",
    "    print \"current w1:\", sess.run([w1, ema.average(w1)])\n",
    "\n",
    "    sess.run(ema_op)\n",
    "    print \"current global_step:\" , sess.run(global_step)\n",
    "    print \"current w1:\", sess.run([w1, ema.average(w1)])\n",
    "\n",
    "    sess.run(ema_op)\n",
    "    print \"current global_step:\" , sess.run(global_step)\n",
    "    print \"current w1:\", sess.run([w1, ema.average(w1)])\n",
    "\n",
    "    sess.run(ema_op)\n",
    "    print \"current global_step:\" , sess.run(global_step)\n",
    "    print \"current w1:\", sess.run([w1, ema.average(w1)])\n",
    "\n",
    "    sess.run(ema_op)\n",
    "    print \"current global_step:\" , sess.run(global_step)\n",
    "    print \"current w1:\", sess.run([w1, ema.average(w1)])\n",
    "\n",
    "    sess.run(ema_op)\n",
    "    print \"current global_step:\" , sess.run(global_step)\n",
    "    print \"current w1:\", sess.run([w1, ema.average(w1)])\n",
    "    \n",
    "\"\"\"\n",
    "运行结果：\n",
    "current global_step: 0\n",
    "current w1 [0.0, 0.0]\n",
    "current global_step: 0\n",
    "current w1 [1.0, 0.9]\n",
    "current global_step: 100\n",
    "current w1: [10.0, 1.6445453]\n",
    "current global_step: 100\n",
    "current w1: [10.0, 2.3281732]\n",
    "current global_step: 100\n",
    "current w1: [10.0, 2.955868]\n",
    "current global_step: 100\n",
    "current w1: [10.0, 3.532206]\n",
    "current global_step: 100\n",
    "current w1: [10.0, 4.061389]\n",
    "current global_step: 100\n",
    "current w1: [10.0, 4.547275]\n",
    "current global_step: 100\n",
    "current w1: [10.0, 4.9934072]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化\n",
    "[Link1](https://blog.csdn.net/u012436149/article/details/70264257)\n",
    "[Link2](https://blog.csdn.net/u011012422/article/details/72808898?utm_source=itdadao&utm_medium=referral)\n",
    "\n",
    "> 正则化在损失函数中引入模型复杂度指标，利用给`W`加权值，弱化了训练数据的噪声(一般不正则化参数`b`)\n",
    "\n",
    "> 正则化计算公式\n",
    ">> loss = loss(y 与 y_) + REGULARIZER * loss(w)\n",
    ">>> * loss(y 与 y_): 模型中所有参数的损失函数, 如: 交叉熵、均方误差\n",
    ">>> * REGULARIZER: 超参数，给出参数`w`在总loss中的比例，即正则化的权重\n",
    ">>> * loss(w): 需要正则化的参数   \n",
    "\n",
    "> L1 正则化\n",
    ">> loss(w) = tf.contrib.layers.l1_regularizer(scale)(w)\n",
    ">>> * scale: 超参数，正则项的系数\n",
    "\n",
    "> L2 正则化\n",
    ">> loss(w) = tf.contrib.layers.l2_regularizer(scale)(w)\n",
    ">>> * scale: 超参数，正则项的系数\n",
    "\n",
    "> 多种正则化组合\n",
    ">> loss(w) = tf.contrib.layers.sum_regularizer(regularizer_list)(w)\n",
    ">>> 返回一个可以执行多种(个)正则化的函数.意思是,创建一个正则化方法,这个方法是多个正则化方法的混合体\n",
    ">>> * regularizer_list: 正则化方法的列表\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def get_weights(shape, regularizer):\n",
    "    var = tf.Variable(tf.random_normal(shape), dtype=tf.float32)\n",
    "    tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(var))\n",
    "    return var\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "batch_size = 8\n",
    "layer_dimension = [2, 10, 10, 10, 1]\n",
    "n_layers = len(layer_dimension)\n",
    "cur_lay = x\n",
    "in_dimension = layer_dimension[0]\n",
    "\n",
    "for i in range(1, n_layers):\n",
    "    out_dimension = layer_dimension[i]\n",
    "    weights = get_weights([in_dimension, out_dimension], 0.001)\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[out_dimension]))\n",
    "    cur_lay = tf.nn.relu(tf.matmul(cur_lay, weights)+bias)\n",
    "    in_dimension = layer_dimension[i]\n",
    "\n",
    "mess_loss = tf.reduce_mean(tf.square(y_-cur_lay))\n",
    "tf.add_to_collection('losses', mess_loss)\n",
    "loss = tf.add_n(tf.get_collection('losses'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络搭建套路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\"\"\"\n",
    "    forwoard.py\n",
    "    前向传播就是搭建神经网络，设计网络结构\n",
    "\"\"\"\n",
    "\n",
    "#定义神经网络的输入、参数和输出，定义前向传播过程 \n",
    "def get_weight(shape, regularizer):\n",
    "    w = tf.Variable(tf.random_normal(shape), dtype=tf.float32)\n",
    "    tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(w))\n",
    "    return w\n",
    "\n",
    "def get_bias(shape):  \n",
    "    b = tf.Variable(tf.constant(0.01, shape=shape)) \n",
    "    return b\n",
    "\n",
    "def forward(x, regularizer):\n",
    "    w1 = get_weight([2,11], regularizer)\t\n",
    "    b1 = get_bias([11])\n",
    "    y1 = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "\n",
    "    w2 = get_weight([11,1], regularizer)\n",
    "    b2 = get_bias([1])\n",
    "    y = tf.matmul(y1, w2) + b2 \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "\n",
    "\"\"\"\n",
    "    backward.py\n",
    "    反向传播就是训练网络，优化网络参数\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import forward\n",
    "\n",
    "STEPS = 40000\n",
    "BATCH_SIZE = 30 \n",
    "LEARNING_RATE_BASE = 0.001\n",
    "LEARNING_RATE_DECAY = 0.999\n",
    "REGULARIZER = 0.01\n",
    "\n",
    "def backward():\n",
    "    x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "    y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "    X, Y_, Y_c = [...], [...], [...]\n",
    "\n",
    "    y = forward(x, REGULARIZER)\n",
    "    \n",
    "    global_step = tf.Variable(0,trainable=False)\n",
    "    \n",
    "    # 定义指数衰减学习率\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        300/BATCH_SIZE,\n",
    "        LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "\n",
    "    #定义损失函数：交叉熵\n",
    "    # loss_mse = tf.reduce_mean(tf.square(y-y_))\n",
    "    loss_mse = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, \n",
    "                                                              labels=tf.argmax(y_, 1))\n",
    "    \n",
    "    loss_total = loss_mse + tf.add_n(tf.get_collection('losses'))\n",
    "    \n",
    "    #定义反向传播方法：包含正则化\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss_total, \n",
    "                                                                global_step=global_step)\n",
    "\n",
    "    #定义滑动平均\n",
    "    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    ema_op = ema.apply(tf.trainable_variables())\n",
    "    with tf.control_dependencies([train_step, ema_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        for i in range(STEPS):\n",
    "            start = (i*BATCH_SIZE) % 300\n",
    "            end = start + BATCH_SIZE\n",
    "            sess.run(train_step, feed_dict={x: X[start:end], y_:Y_[start:end]})\n",
    "            if i % 2000 == 0:\n",
    "                loss_v = sess.run(loss_total, feed_dict={x:X,y_:Y_})\n",
    "                print \"After %d steps, loss is: %f\" %(i, loss_v)\n",
    "\n",
    "        xx, yy = np.mgrid[-3:3:.01, -3:3:.01]\n",
    "        grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "        probs = sess.run(y, feed_dict={x:grid})\n",
    "        probs = probs.reshape(xx.shape)\n",
    "    \n",
    "    plt.scatter(X[:,0], X[:,1], c=np.squeeze(Y_c)) \n",
    "    plt.contour(xx, yy, probs, levels=[.5])\n",
    "    plt.show()\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手写数字识别示例(MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\"\"\"\n",
    "    filename: mnist_forward.py\n",
    "    定义神经网络模型\n",
    "\"\"\"\n",
    "\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "LAYER1_NODE = 500\n",
    "\n",
    "def get_weight(shape, regularizer):\n",
    "    w = tf.Variable(tf.truncated_normal(shape,stddev=0.1))\n",
    "    if regularizer != None: tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(w))\n",
    "    return w\n",
    "\n",
    "\n",
    "def get_bias(shape):  \n",
    "    b = tf.Variable(tf.zeros(shape))  \n",
    "    return b\n",
    "\n",
    "def forward(x, regularizer):\n",
    "    w1 = get_weight([INPUT_NODE, LAYER1_NODE], regularizer)\n",
    "    b1 = get_bias([LAYER1_NODE])\n",
    "    y1 = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "\n",
    "    w2 = get_weight([LAYER1_NODE, OUTPUT_NODE], regularizer)\n",
    "    b2 = get_bias([OUTPUT_NODE])\n",
    "    \n",
    "    # 注：此处不再经过激活函数; 因为后面需要使用softmax函数，它也属于激活函数\n",
    "    y = tf.matmul(y1, w2) + b2    \n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import mnist_forward\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "    filename: mnist_backward.py\n",
    "    训练神经网络\n",
    "\"\"\"\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "LEARNING_RATE_BASE = 0.1\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZER = 0.0001\n",
    "STEPS = 50000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "MODEL_SAVE_PATH=\"./model/\"\n",
    "MODEL_NAME=\"mnist_model\"\n",
    "\n",
    "\n",
    "def backward(mnist):\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, mnist_forward.INPUT_NODE])\n",
    "    y_ = tf.placeholder(tf.float32, [None, mnist_forward.OUTPUT_NODE])\n",
    "    y = mnist_forward.forward(x, REGULARIZER)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    cem = tf.reduce_mean(ce)\n",
    "    loss = cem + tf.add_n(tf.get_collection('losses'))\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples / BATCH_SIZE, \n",
    "        LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    ema_op = ema.apply(tf.trainable_variables())\n",
    "    with tf.control_dependencies([train_step, ema_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "\n",
    "        # 支持从断点处恢复训练\n",
    "        ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "        for i in range(STEPS):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "            if i % 1000 == 0:\n",
    "                print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))\n",
    "                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n",
    "\n",
    "def main():\n",
    "    mnist = input_data.read_data_sets(\"./data/\", one_hot=True)\n",
    "    backward(mnist)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import mnist_forward\n",
    "import mnist_backward\n",
    "TEST_INTERVAL_SECS = 5\n",
    "\n",
    "\"\"\"\n",
    "    filename: mnist_test.py\n",
    "    神经网络测试\n",
    "\"\"\"\n",
    "\n",
    "def test(mnist):\n",
    "    with tf.Graph().as_default() as g:\n",
    "        x = tf.placeholder(tf.float32, [None, mnist_forward.INPUT_NODE])\n",
    "        y_ = tf.placeholder(tf.float32, [None, mnist_forward.OUTPUT_NODE])\n",
    "        y = mnist_forward.forward(x, None)\n",
    "\n",
    "        ema = tf.train.ExponentialMovingAverage(mnist_backward.MOVING_AVERAGE_DECAY)\n",
    "        ema_restore = ema.variables_to_restore()\n",
    "        saver = tf.train.Saver(ema_restore)\n",
    "\t\t\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        while True:\n",
    "            with tf.Session() as sess:\n",
    "                ckpt = tf.train.get_checkpoint_state(mnist_backward.MODEL_SAVE_PATH)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                    global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "                    accuracy_score = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "                    print(\"After %s training step(s), test accuracy = %g\" % (global_step, accuracy_score))\n",
    "                else:\n",
    "                    print('No checkpoint file found')\n",
    "                    return\n",
    "            time.sleep(TEST_INTERVAL_SECS)\n",
    "\n",
    "def main():\n",
    "    mnist = input_data.read_data_sets(\"./data/\", one_hot=True)\n",
    "    test(mnist)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "272px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
