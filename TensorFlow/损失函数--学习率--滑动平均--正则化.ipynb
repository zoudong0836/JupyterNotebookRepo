{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#NN-(Neural-Network)-神经网络\" data-toc-modified-id=\"NN-(Neural-Network)-神经网络-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>NN (Neural Network) 神经网络</a></span></li><li><span><a href=\"#损失函数(loss)\" data-toc-modified-id=\"损失函数(loss)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>损失函数(loss)</a></span><ul class=\"toc-item\"><li><span><a href=\"#均方误差损失函数\" data-toc-modified-id=\"均方误差损失函数-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>均方误差损失函数</a></span></li><li><span><a href=\"#自定义损失函数\" data-toc-modified-id=\"自定义损失函数-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>自定义损失函数</a></span></li><li><span><a href=\"#交叉熵损失函数\" data-toc-modified-id=\"交叉熵损失函数-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>交叉熵损失函数</a></span></li><li><span><a href=\"#损失函数示例\" data-toc-modified-id=\"损失函数示例-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>损失函数示例</a></span></li></ul></li><li><span><a href=\"#学习率(learning_rate)\" data-toc-modified-id=\"学习率(learning_rate)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>学习率(learning_rate)</a></span></li><li><span><a href=\"#滑动平均(影子值)\" data-toc-modified-id=\"滑动平均(影子值)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>滑动平均(影子值)</a></span></li><li><span><a href=\"#正则化\" data-toc-modified-id=\"正则化-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>正则化</a></span></li><li><span><a href=\"#神经网络搭建套路\" data-toc-modified-id=\"神经网络搭建套路-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>神经网络搭建套路</a></span><ul class=\"toc-item\"><li><span><a href=\"#前向传播，搭建神经网络\" data-toc-modified-id=\"前向传播，搭建神经网络-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>前向传播，搭建神经网络</a></span></li><li><span><a href=\"#反向传播，训练神经网络\" data-toc-modified-id=\"反向传播，训练神经网络-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>反向传播，训练神经网络</a></span></li></ul></li><li><span><a href=\"#手写数字识别示例(MNIST)\" data-toc-modified-id=\"手写数字识别示例(MNIST)-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>手写数字识别示例(MNIST)</a></span><ul class=\"toc-item\"><li><span><a href=\"#定义神经网络模型(mnist_forward.py)\" data-toc-modified-id=\"定义神经网络模型(mnist_forward.py)-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>定义神经网络模型(mnist_forward.py)</a></span></li><li><span><a href=\"#训练神经网络(minist_backward.py)\" data-toc-modified-id=\"训练神经网络(minist_backward.py)-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>训练神经网络(minist_backward.py)</a></span></li><li><span><a href=\"#测试神经网络(mnist_test.py)\" data-toc-modified-id=\"测试神经网络(mnist_test.py)-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>测试神经网络(mnist_test.py)</a></span></li><li><span><a href=\"#应用神经网络(mnist_app.py)\" data-toc-modified-id=\"应用神经网络(mnist_app.py)-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>应用神经网络(mnist_app.py)</a></span></li></ul></li><li><span><a href=\"#制作数据集(tfrecord格式)\" data-toc-modified-id=\"制作数据集(tfrecord格式)-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>制作数据集(tfrecord格式)</a></span><ul class=\"toc-item\"><li><span><a href=\"#将数据保存为tfrecord格式\" data-toc-modified-id=\"将数据保存为tfrecord格式-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>将数据保存为tfrecord格式</a></span><ul class=\"toc-item\"><li><span><a href=\"#创建tfrecord文件\" data-toc-modified-id=\"创建tfrecord文件-8.1.1\"><span class=\"toc-item-num\">8.1.1&nbsp;&nbsp;</span>创建tfrecord文件</a></span></li><li><span><a href=\"#写入数据到tfrecord\" data-toc-modified-id=\"写入数据到tfrecord-8.1.2\"><span class=\"toc-item-num\">8.1.2&nbsp;&nbsp;</span>写入数据到tfrecord</a></span></li><li><span><a href=\"#读取tfrecord数据\" data-toc-modified-id=\"读取tfrecord数据-8.1.3\"><span class=\"toc-item-num\">8.1.3&nbsp;&nbsp;</span>读取tfrecord数据</a></span></li><li><span><a href=\"#tfrecord-示例\" data-toc-modified-id=\"tfrecord-示例-8.1.4\"><span class=\"toc-item-num\">8.1.4&nbsp;&nbsp;</span><a href=\"https://blog.csdn.net/happyhorizion/article/details/77894055\" target=\"_blank\">tfrecord 示例</a></a></span></li></ul></li></ul></li><li><span><a href=\"#CNN(Convolutional-Neural-Network)\" data-toc-modified-id=\"CNN(Convolutional-Neural-Network)-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>CNN(Convolutional Neural Network)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Convolutional-(卷积)\" data-toc-modified-id=\"Convolutional-(卷积)-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Convolutional (卷积)</a></span></li><li><span><a href=\"#Pooling-(池化)\" data-toc-modified-id=\"Pooling-(池化)-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Pooling (池化)</a></span></li><li><span><a href=\"#Dropout-(舍弃)\" data-toc-modified-id=\"Dropout-(舍弃)-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Dropout (舍弃)</a></span></li><li><span><a href=\"#Lenet-5-示例\" data-toc-modified-id=\"Lenet-5-示例-9.4\"><span class=\"toc-item-num\">9.4&nbsp;&nbsp;</span>Lenet-5 示例</a></span><ul class=\"toc-item\"><li><span><a href=\"#定义神经网络模型(lenet5_forward.py)\" data-toc-modified-id=\"定义神经网络模型(lenet5_forward.py)-9.4.1\"><span class=\"toc-item-num\">9.4.1&nbsp;&nbsp;</span>定义神经网络模型(lenet5_forward.py)</a></span></li><li><span><a href=\"#训练神经网络(lenet5_backward.py)\" data-toc-modified-id=\"训练神经网络(lenet5_backward.py)-9.4.2\"><span class=\"toc-item-num\">9.4.2&nbsp;&nbsp;</span>训练神经网络(lenet5_backward.py)</a></span></li><li><span><a href=\"#测试神经网络模型(lenet5_test.py)\" data-toc-modified-id=\"测试神经网络模型(lenet5_test.py)-9.4.3\"><span class=\"toc-item-num\">9.4.3&nbsp;&nbsp;</span>测试神经网络模型(lenet5_test.py)</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN (Neural Network) 神经网络\n",
    "> 全连接NN: 每个神经元与前后相邻层的每一个神经元都有连接关系，输入是特征，输出为预测的结果\n",
    ">> 参数个数： Σ(前层 x 后层 + 后层)\n",
    "\n",
    "> NN复杂度： 常用NN层数和NN参数的个数表示\n",
    ">> * 层数 = 隐藏层的层数 + 1个输出层 (输入层不纳入计算)\n",
    ">> * 总参数 = 总W + 总b  \n",
    ">>> 每一层的 `W` 个数等于 `上一层神经元个数` x `本层神经元个数`    \n",
    ">>> 每一层的 `b` 个数等于 `本层神经元个数`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数(loss)\n",
    "[Link1](https://blog.csdn.net/marsjhao/article/details/72630147)\n",
    "\n",
    "> 预测值(y)与已知答案(y_)的差距  \n",
    "\n",
    "> NN优化的目标: loss最小化\n",
    ">> 1. mse (Mean Squared Error) 均方误差\n",
    ">>> loss_mse = tf.reduce_mean(tf.square(y_ - y))\n",
    ">> 2. 自定义\n",
    ">>> loss = tf.reduce_sum(tf.where(tf.greater(y, y_), arg1*(y - y_), arg2*(y_ - y)))\n",
    ">> 3. ce (Cross Entropy) 交叉熵 -- `表征两个概率分布之间的距离`\n",
    ">>> ce = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-12, 1.0)))   \n",
    ">>>> tf.clip_by_value()函数可将一个tensor的元素数值限制在指定范围内，这样可防止一些错误运算，起到数值检查作用\n",
    "\n",
    "> TensorFlow 提供了集成交叉熵函数  \n",
    ">>> * ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits_v2(logits=y, labels=y_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 均方误差损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "\n",
    "\"\"\"\n",
    "  '均方误差' 损失函数\n",
    "\"\"\"\n",
    "\n",
    "#0导入模块，生成数据集\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "BATCH_SIZE = 8\n",
    "SEED = 23455\n",
    "\n",
    "rdm = np.random.RandomState(SEED)\n",
    "X = rdm.rand(32,2)\n",
    "Y_ = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1, x2) in X]\n",
    "\n",
    "#1定义神经网络的输入、参数和输出，定义前向传播过程。\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "w1= tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1))\n",
    "y = tf.matmul(x, w1)\n",
    "\n",
    "#2定义损失函数及反向传播方法。\n",
    "#定义损失函数为MSE,反向传播方法为梯度下降。\n",
    "loss_mse = tf.reduce_mean(tf.square(y_ - y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss_mse)\n",
    "\n",
    "#3生成会话，训练STEPS轮\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 20000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = (i*BATCH_SIZE) % 32 + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y_[start:end]})\n",
    "        if i % 500 == 0:\n",
    "            print \"After %d training steps, w1 is: \" % (i)\n",
    "            print sess.run(w1), \"\\n\"\n",
    "    print \"Final w1 is: \\n\", sess.run(w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "\n",
    "\"\"\"\n",
    "  '自定义' 损失函数\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "BATCH_SIZE = 8\n",
    "SEED = 23455\n",
    "COST = 1     # 酸奶成本1元\n",
    "PROFIT = 9   # 酸奶利润9元\n",
    "#预测少了损失大，故不要预测少，故生成的模型会多预测一些\n",
    "\n",
    "rdm = np.random.RandomState(SEED)\n",
    "X = rdm.rand(32,2)\n",
    "Y = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1, x2) in X]\n",
    "\n",
    "#1定义神经网络的输入、参数和输出，定义前向传播过程。\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "w1= tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1))\n",
    "y = tf.matmul(x, w1)\n",
    "\n",
    "#2定义损失函数及反向传播方法。\n",
    "# 定义损失函数使得预测少了的损失大，于是模型应该偏向多的方向预测。\n",
    "loss = tf.reduce_sum(tf.where(tf.greater(y, y_), (y - y_)*COST, (y_ - y)*PROFIT))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "\n",
    "#3生成会话，训练STEPS轮\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 3000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = (i*BATCH_SIZE) % 32 + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "        if i % 500 == 0:\n",
    "            print \"After %d training steps, w1 is: \" % (i)\n",
    "            print sess.run(w1), \"\\n\"\n",
    "    print \"Final w1 is: \\n\", sess.run(w1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉熵损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "\n",
    "\"\"\"\n",
    "  '交叉熵' 损失函数\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "BATCH_SIZE = 8\n",
    "SEED = 23455\n",
    "\n",
    "rdm = np.random.RandomState(SEED)\n",
    "X = rdm.rand(32,2)\n",
    "Y = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1, x2) in X]\n",
    "\n",
    "#1定义神经网络的输入、参数和输出，定义前向传播过程。\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "w1= tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1))\n",
    "y = tf.matmul(x, w1)\n",
    "\n",
    "#2定义损失函数及反向传播方法。\n",
    "#重新定义损失函数，使得预测多了的损失大，于是模型应该偏向少的方向预测。\n",
    "'''\n",
    "** https://blog.csdn.net/m0_37041325/article/details/77043598\n",
    "1. 这个函数和tf.nn.softmax_cross_entropy_with_logits函数比较明显的区别在于它的参数labels的不同，\n",
    "这里的参数label是非稀疏表示的\n",
    "2. 稀疏表示的形式为[0,0,1](one-hot vector), 这个表示这个样本为第3个分类;\n",
    "而非稀疏表示就表示为2(scala)（因为从0开始算，0,1,2,就能表示三类）\n",
    "3. tf.nn.sparse_softmax_cross_entropy_with_logits函数\n",
    "比tf.nn.softmax_cross_entropy_with_logits多了一步操作，将labels稀疏化的操作\n",
    "'''\n",
    "ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "loss = tf.reduce_mean(ce)\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "\n",
    "#3生成会话，训练STEPS轮\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 10000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = (i*BATCH_SIZE) % 32 + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "        if i % 500 == 0:\n",
    "            print \"After %d training steps, w1 is: \" % (i)\n",
    "            print sess.run(w1), \"\\n\"\n",
    "    print \"Final w1 is: \\n\", sess.run(w1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "\n",
    "\"\"\"\n",
    "  'sparse_softmax_cross_entropy_with_logits' 函数示例\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "y2 = tf.convert_to_tensor([[0, 0, 1, 0]], dtype=tf.int64)\n",
    "y_2 = tf.convert_to_tensor([[-2.6, -1.7, 3.2, 0.1]], dtype=tf.float32)\n",
    "y_2_2 = tf.argmax(y2, 1)\n",
    "c2 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_2, labels=y_2_2)\n",
    "\n",
    "y3 = tf.convert_to_tensor([[0, 0, 1, 0], [0, 0, 1, 0]], dtype=tf.int64)\n",
    "y_3 = tf.convert_to_tensor([[-2.6, -1.7, -3.2, 0.1], [-2.6, -1.7, 3.2, 0.1]], dtype=tf.float32)\n",
    "y_3_3 = tf.argmax(y_3, 1)\n",
    "c3 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_3, labels=y_3_3)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    y2_ = sess.run(y2)\n",
    "    print y2_.shape, y2_        # (1, 4) [[0 0 1 0]]\n",
    "\n",
    "    y_2_ = sess.run(y_2)\n",
    "    print y_2_.shape, y_2_      # (1, 4) [[-2.6 -1.7  3.2  0.1]]\n",
    "\n",
    "    y_2_2_ = sess.run(y_2_2)\n",
    "    print y_2_2_.shape, y_2_2_  # (1,) [2]\n",
    "\n",
    "    c2_ = sess.run(c2)\n",
    "    print c2_.shape, c2_        # (1,) [0.05403664]\n",
    "\n",
    "    y3_ = sess.run(y3)\n",
    "    print y3_.shape, y3_\n",
    "\n",
    "    y_3_ = sess.run(y_3)\n",
    "    print y_3_.shape, y_3_\n",
    "\n",
    "    y_3_3 = sess.run(y_3_3)\n",
    "    print y_3_3.shape, y_3_3\n",
    "\n",
    "    c3_ = sess.run(c3)\n",
    "    print c3_.shape, c3_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习率(learning_rate)\n",
    "[Link1](https://www.imooc.com/article/details/id/27808)\n",
    "\n",
    "> 在神经网络训练过程中，参数每次更新的幅度\n",
    "\n",
    "> 学习率设置大了会震荡不收敛，学习率设置小了收敛速度慢  \n",
    ">> tf.train.GradientDescentOptimizer(learning_rate)  \n",
    "\n",
    "> 指数衰减计算公式\n",
    ">> decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n",
    ">>> * decayed_learning_rate: 每一轮优化时使用的学习率\n",
    ">>> * learning_rate: 超参数，事先设定(预估)的初始学习率\n",
    ">>> * decay_rate: 超参数，衰减系数\n",
    ">>> * decay_steps: 衰减速度 (即迭代多少次进行衰减)\n",
    ">>> * 一般来说，初始学习率、衰减系数和衰减速度都是根据经验设置的\n",
    "\n",
    "> 指数衰减学习率\n",
    ">> * global_step = tf.Variable(0, trainable=False)\n",
    ">>> 迭代次数初始值为0\n",
    ">> * learning_rate = tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate, staircase)\n",
    ">>> * 参数decay_steps=100时，即表示100轮迭代后进行一次衰减\n",
    ">>> * 参数staircase=True时，global_step/decay_steps会被转化为整数，这使得学习率呈阶梯型下降；若为False时，则是连续型下降\n",
    ">> * tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    ">>> 使用指数衰减的学习率，在minimize函数中传入global_step，它将自动更新，learning_rate也随即被更新\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "#设损失函数 loss=(w+1)^2, 令w初值是常数10。反向传播就是求最优w，即求最小loss对应的w值\n",
    "#使用指数衰减的学习率，在迭代初期得到较高的下降速度，可以在较小的训练轮数下取得更有收敛度。\n",
    "import tensorflow as tf\n",
    "\n",
    "LEARNING_RATE_BASE = 0.1   #最初学习率\n",
    "LEARNING_RATE_DECAY = 0.99 #学习率衰减率\n",
    "LEARNING_RATE_STEP = 1     #喂入多少轮BATCH_SIZE后，更新一次学习率，一般设为：总样本数/BATCH_SIZE\n",
    "\n",
    "#运行了几轮BATCH_SIZE的计数器，初值给0, 设为不被训练\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "#定义指数下降学习率\n",
    "learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, \n",
    "                                           global_step, \n",
    "                                           LEARNING_RATE_STEP, \n",
    "                                           LEARNING_RATE_DECAY, \n",
    "                                           staircase=True)\n",
    "#定义待优化参数，初值给10\n",
    "w = tf.Variable(tf.constant(5, dtype=tf.float32))\n",
    "#定义损失函数loss\n",
    "loss = tf.square(w+1)\n",
    "#定义反向传播方法\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate)\\\n",
    "                                            .minimize(loss, global_step=global_step)\n",
    "#生成会话，训练40轮\n",
    "with tf.Session() as sess:\n",
    "    init_op=tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for i in range(40):\n",
    "        sess.run(train_step)\n",
    "        learning_rate_val = sess.run(learning_rate)\n",
    "        global_step_val = sess.run(global_step)\n",
    "        w_val = sess.run(w)\n",
    "        loss_val = sess.run(loss)\n",
    "        print \"After %s steps: global_step is %f, w is %f, learning rate is %f, loss is %f\" % (i, global_step_val, w_val, learning_rate_val, loss_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 滑动平均(影子值)\n",
    "[Link1](https://blog.csdn.net/lanchunhui/article/details/70803060)\n",
    "\n",
    "> 记录了每个参数一段时间内过往值的平均，增加了模型的泛化性   \n",
    "> 滑动平均是针对所有参数：`W`和`b` \n",
    "\n",
    "> 滑动平均计算公式\n",
    ">> * 影子 = 衰减率 x 影子 + (1 - 衰减率) × 参数(W)     （影子初值 = 参数初值）   \n",
    ">> * 衰减率 = min{ MOVING_AVERAVG_DECAY, (1+轮数)/(10+轮数)}\n",
    "\n",
    "> 滑动平均函数\n",
    ">> * global_step = tf.Variable(0, trainable=False)\n",
    ">> * ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    ">> * ema_op = ema.apply(tf.trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import tensorflow as tf\n",
    "\n",
    "#1. 定义变量及滑动平均类\n",
    "#定义一个32位浮点变量，初始值为0.0  这个代码就是不断更新w1参数，优化w1参数，滑动平均做了个w1的影子\n",
    "w1 = tf.Variable(0, dtype=tf.float32)\n",
    "#定义num_updates（NN的迭代轮数）,初始值为0，不可被优化（训练），这个参数不训练\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "#实例化滑动平均类，给衰减率为0.99，当前轮数global_step\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "#ema.apply后的括号里是更新列表，每次运行sess.run（ema_op）时，对更新列表中的元素求滑动平均值。\n",
    "#在实际应用中会使用tf.trainable_variables()自动将所有待训练的参数汇总为列表\n",
    "#ema_op = ema.apply([w1])\n",
    "ema_op = ema.apply(tf.trainable_variables())\n",
    "\n",
    "#2. 查看不同迭代中变量取值的变化。\n",
    "with tf.Session() as sess:\n",
    "    # 初始化\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    #用 ema.average(w1)获取w1滑动平均值 （要运行多个节点，作为列表中的元素列出，写在sess.run中）\n",
    "    #打印出当前参数w1和w1滑动平均值\n",
    "    print \"current global_step:\", sess.run(global_step)\n",
    "    print \"current w1\", sess.run([w1, ema.average(w1)]) \n",
    "    \n",
    "    # 参数w1的值赋为1\n",
    "    sess.run(tf.assign(w1, 1))\n",
    "    sess.run(ema_op)\n",
    "    print \"current global_step:\", sess.run(global_step)\n",
    "    print \"current w1\", sess.run([w1, ema.average(w1)]) \n",
    "    \n",
    "    # 更新global_step和w1的值,模拟出轮数为100时，参数w1变为10, \n",
    "    # 以下代码global_step保持为100，每次执行滑动平均操作，影子值会更新 \n",
    "    sess.run(tf.assign(global_step, 100))  \n",
    "    sess.run(tf.assign(w1, 10))\n",
    "    sess.run(ema_op)\n",
    "    print \"current global_step:\", sess.run(global_step)\n",
    "    print \"current w1:\", sess.run([w1, ema.average(w1)])       \n",
    "    \n",
    "    # 每次sess.run会更新一次w1的滑动平均值\n",
    "    sess.run(ema_op)\n",
    "    print \"current global_step:\" , sess.run(global_step)\n",
    "    print \"current w1:\", sess.run([w1, ema.average(w1)])\n",
    "\n",
    "    sess.run(ema_op)\n",
    "    print \"current global_step:\" , sess.run(global_step)\n",
    "    print \"current w1:\", sess.run([w1, ema.average(w1)])\n",
    "\n",
    "    sess.run(ema_op)\n",
    "    print \"current global_step:\" , sess.run(global_step)\n",
    "    print \"current w1:\", sess.run([w1, ema.average(w1)])\n",
    "\n",
    "    sess.run(ema_op)\n",
    "    print \"current global_step:\" , sess.run(global_step)\n",
    "    print \"current w1:\", sess.run([w1, ema.average(w1)])\n",
    "\n",
    "    sess.run(ema_op)\n",
    "    print \"current global_step:\" , sess.run(global_step)\n",
    "    print \"current w1:\", sess.run([w1, ema.average(w1)])\n",
    "\n",
    "    sess.run(ema_op)\n",
    "    print \"current global_step:\" , sess.run(global_step)\n",
    "    print \"current w1:\", sess.run([w1, ema.average(w1)])\n",
    "    \n",
    "\"\"\"\n",
    "运行结果：\n",
    "current global_step: 0\n",
    "current w1 [0.0, 0.0]\n",
    "current global_step: 0\n",
    "current w1 [1.0, 0.9]\n",
    "current global_step: 100\n",
    "current w1: [10.0, 1.6445453]\n",
    "current global_step: 100\n",
    "current w1: [10.0, 2.3281732]\n",
    "current global_step: 100\n",
    "current w1: [10.0, 2.955868]\n",
    "current global_step: 100\n",
    "current w1: [10.0, 3.532206]\n",
    "current global_step: 100\n",
    "current w1: [10.0, 4.061389]\n",
    "current global_step: 100\n",
    "current w1: [10.0, 4.547275]\n",
    "current global_step: 100\n",
    "current w1: [10.0, 4.9934072]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化\n",
    "[Link1](https://blog.csdn.net/u012436149/article/details/70264257)\n",
    "[Link2](https://blog.csdn.net/u011012422/article/details/72808898?utm_source=itdadao&utm_medium=referral)\n",
    "\n",
    "> 正则化在损失函数中引入模型复杂度指标，利用给`W`加权值，弱化了训练数据的噪声(一般不正则化参数`b`)\n",
    "\n",
    "> 正则化计算公式\n",
    ">> loss = loss(y 与 y_) + REGULARIZER * loss(w)\n",
    ">>> * loss(y 与 y_): 模型中所有参数的损失函数, 如: 交叉熵、均方误差\n",
    ">>> * REGULARIZER: 超参数，给出参数`w`在总loss中的比例，即正则化的权重\n",
    ">>> * loss(w): 需要正则化的参数   \n",
    "\n",
    "> L1 正则化\n",
    ">> loss(w) = tf.contrib.layers.l1_regularizer(scale)(w)\n",
    ">>> * scale: 超参数，正则项的系数\n",
    "\n",
    "> L2 正则化\n",
    ">> loss(w) = tf.contrib.layers.l2_regularizer(scale)(w)\n",
    ">>> * scale: 超参数，正则项的系数\n",
    "\n",
    "> 多种正则化组合\n",
    ">> loss(w) = tf.contrib.layers.sum_regularizer(regularizer_list)(w)\n",
    ">>> 返回一个可以执行多种(个)正则化的函数.意思是,创建一个正则化方法,这个方法是多个正则化方法的混合体\n",
    ">>> * regularizer_list: 正则化方法的列表\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def get_weights(shape, regularizer):\n",
    "    var = tf.Variable(tf.random_normal(shape), dtype=tf.float32)\n",
    "    tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(var))\n",
    "    return var\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "batch_size = 8\n",
    "layer_dimension = [2, 10, 10, 10, 1]\n",
    "n_layers = len(layer_dimension)\n",
    "cur_lay = x\n",
    "in_dimension = layer_dimension[0]\n",
    "\n",
    "for i in range(1, n_layers):\n",
    "    out_dimension = layer_dimension[i]\n",
    "    weights = get_weights([in_dimension, out_dimension], 0.001)\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[out_dimension]))\n",
    "    cur_lay = tf.nn.relu(tf.matmul(cur_lay, weights)+bias)\n",
    "    in_dimension = layer_dimension[i]\n",
    "\n",
    "mess_loss = tf.reduce_mean(tf.square(y_-cur_lay))\n",
    "tf.add_to_collection('losses', mess_loss)\n",
    "loss = tf.add_n(tf.get_collection('losses'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络搭建套路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前向传播，搭建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\"\"\"\n",
    "    forwoard.py\n",
    "    前向传播就是搭建神经网络，设计网络结构\n",
    "\"\"\"\n",
    "\n",
    "#定义神经网络的输入、参数和输出，定义前向传播过程 \n",
    "def get_weight(shape, regularizer):\n",
    "    w = tf.Variable(tf.random_normal(shape), dtype=tf.float32)\n",
    "    tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(w))\n",
    "    return w\n",
    "\n",
    "def get_bias(shape):  \n",
    "    b = tf.Variable(tf.constant(0.01, shape=shape)) \n",
    "    return b\n",
    "\n",
    "def forward(x, regularizer):\n",
    "    w1 = get_weight([2,11], regularizer)\t\n",
    "    b1 = get_bias([11])\n",
    "    y1 = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "\n",
    "    w2 = get_weight([11,1], regularizer)\n",
    "    b2 = get_bias([1])\n",
    "    y = tf.matmul(y1, w2) + b2 \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反向传播，训练神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "\n",
    "\"\"\"\n",
    "    backward.py\n",
    "    反向传播就是训练网络，优化网络参数\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import forward\n",
    "\n",
    "STEPS = 40000\n",
    "BATCH_SIZE = 30 \n",
    "LEARNING_RATE_BASE = 0.001\n",
    "LEARNING_RATE_DECAY = 0.999\n",
    "REGULARIZER = 0.01\n",
    "\n",
    "def backward():\n",
    "    x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "    y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "    X, Y_, Y_c = [...], [...], [...]\n",
    "\n",
    "    y = forward(x, REGULARIZER)\n",
    "    \n",
    "    global_step = tf.Variable(0,trainable=False)\n",
    "    \n",
    "    # 定义指数衰减学习率\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        300/BATCH_SIZE,\n",
    "        LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "\n",
    "    #定义损失函数：交叉熵\n",
    "    # loss_mse = tf.reduce_mean(tf.square(y-y_))\n",
    "    loss_mse = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, \n",
    "                                                              labels=tf.argmax(y_, 1))\n",
    "    \n",
    "    loss_total = loss_mse + tf.add_n(tf.get_collection('losses'))\n",
    "    \n",
    "    #定义反向传播方法：包含正则化\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss_total, \n",
    "                                                                global_step=global_step)\n",
    "\n",
    "    #定义滑动平均\n",
    "    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    ema_op = ema.apply(tf.trainable_variables())\n",
    "    with tf.control_dependencies([train_step, ema_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        for i in range(STEPS):\n",
    "            start = (i*BATCH_SIZE) % 300\n",
    "            end = start + BATCH_SIZE\n",
    "            sess.run(train_step, feed_dict={x: X[start:end], y_:Y_[start:end]})\n",
    "            if i % 2000 == 0:\n",
    "                loss_v = sess.run(loss_total, feed_dict={x:X,y_:Y_})\n",
    "                print \"After %d steps, loss is: %f\" %(i, loss_v)\n",
    "\n",
    "        xx, yy = np.mgrid[-3:3:.01, -3:3:.01]\n",
    "        grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "        probs = sess.run(y, feed_dict={x:grid})\n",
    "        probs = probs.reshape(xx.shape)\n",
    "    \n",
    "    plt.scatter(X[:,0], X[:,1], c=np.squeeze(Y_c)) \n",
    "    plt.contour(xx, yy, probs, levels=[.5])\n",
    "    plt.show()\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手写数字识别示例(MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义神经网络模型(mnist_forward.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import tensorflow as tf\n",
    "\n",
    "\"\"\"\n",
    "    filename: mnist_forward.py\n",
    "    定义神经网络模型\n",
    "\"\"\"\n",
    "\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "LAYER1_NODE = 500\n",
    "\n",
    "def get_weight(shape, regularizer):\n",
    "    w = tf.Variable(tf.truncated_normal(shape,stddev=0.1))\n",
    "    if regularizer != None: tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(w))\n",
    "    return w\n",
    "\n",
    "\n",
    "def get_bias(shape):  \n",
    "    b = tf.Variable(tf.zeros(shape))  \n",
    "    return b\n",
    "\n",
    "def forward(x, regularizer):\n",
    "    w1 = get_weight([INPUT_NODE, LAYER1_NODE], regularizer)\n",
    "    b1 = get_bias([LAYER1_NODE])\n",
    "    y1 = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "\n",
    "    w2 = get_weight([LAYER1_NODE, OUTPUT_NODE], regularizer)\n",
    "    b2 = get_bias([OUTPUT_NODE])\n",
    "    \n",
    "    # 注：此处不再经过激活函数; 因为后面需要使用softmax函数，它也属于激活函数\n",
    "    y = tf.matmul(y1, w2) + b2    \n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练神经网络(minist_backward.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "\n",
    "\"\"\"\n",
    "    filename: mnist_backward.py\n",
    "    训练神经网络\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import mnist_forward\n",
    "import os\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "LEARNING_RATE_BASE = 0.1\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZER = 0.0001\n",
    "STEPS = 50000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "MODEL_SAVE_PATH=\"./model/\"\n",
    "MODEL_NAME=\"mnist_model\"\n",
    "\n",
    "\n",
    "def backward(mnist):\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, mnist_forward.INPUT_NODE])\n",
    "    y_ = tf.placeholder(tf.float32, [None, mnist_forward.OUTPUT_NODE])\n",
    "    y = mnist_forward.forward(x, REGULARIZER)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    cem = tf.reduce_mean(ce)\n",
    "    loss = cem + tf.add_n(tf.get_collection('losses'))\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples / BATCH_SIZE, \n",
    "        LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    ema_op = ema.apply(tf.trainable_variables())\n",
    "    with tf.control_dependencies([train_step, ema_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "\n",
    "        # 支持从断点处恢复训练\n",
    "        ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "        for i in range(STEPS):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "            if i % 1000 == 0:\n",
    "                print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))\n",
    "                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n",
    "\n",
    "def main():\n",
    "    mnist = input_data.read_data_sets(\"./data/\", one_hot=True)\n",
    "    backward(mnist)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试神经网络(mnist_test.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "\n",
    "\"\"\"\n",
    "    filename: mnist_test.py\n",
    "    神经网络测试\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import mnist_forward\n",
    "import mnist_backward\n",
    "TEST_INTERVAL_SECS = 5\n",
    "\n",
    "def test(mnist):\n",
    "    with tf.Graph().as_default() as g:\n",
    "        x = tf.placeholder(tf.float32, [None, mnist_forward.INPUT_NODE])\n",
    "        y_ = tf.placeholder(tf.float32, [None, mnist_forward.OUTPUT_NODE])\n",
    "        y = mnist_forward.forward(x, None)\n",
    "\n",
    "        ema = tf.train.ExponentialMovingAverage(mnist_backward.MOVING_AVERAGE_DECAY)\n",
    "        ema_restore = ema.variables_to_restore()\n",
    "        saver = tf.train.Saver(ema_restore)\n",
    "\t\t\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        while True:\n",
    "            with tf.Session() as sess:\n",
    "                ckpt = tf.train.get_checkpoint_state(mnist_backward.MODEL_SAVE_PATH)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                    global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "                    accuracy_score = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "                    print(\"After %s training step(s), test accuracy = %g\" % (global_step, accuracy_score))\n",
    "                else:\n",
    "                    print('No checkpoint file found')\n",
    "                    return\n",
    "            time.sleep(TEST_INTERVAL_SECS)\n",
    "\n",
    "def main():\n",
    "    mnist = input_data.read_data_sets(\"./data/\", one_hot=True)\n",
    "    test(mnist)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 应用神经网络(mnist_app.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "\n",
    "\"\"\"\n",
    "    filename: mnist_app.py\n",
    "    神经网络应用\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import mnist_backward\n",
    "import mnist_forward\n",
    "\n",
    "def restore_model(testPicArr):\n",
    "    with tf.Graph().as_default() as tg:\n",
    "        x = tf.placeholder(tf.float32, [None, mnist_forward.INPUT_NODE])\n",
    "        y = mnist_forward.forward(x, None)\n",
    "        \n",
    "        # 返回概率最高的索引 (索引正好对应数字内容)\n",
    "        preValue = tf.argmax(y, 1)\n",
    "\n",
    "        # 恢复滑动平均值参数\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(mnist_backward.MOVING_AVERAGE_DECAY)\n",
    "        variables_to_restore = variable_averages.variables_to_restore()\n",
    "        saver = tf.train.Saver(variables_to_restore)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            ckpt = tf.train.get_checkpoint_state(mnist_backward.MODEL_SAVE_PATH)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "                preValue = sess.run(preValue, feed_dict={x:testPicArr})\n",
    "                return preValue\n",
    "            else:\n",
    "                print(\"No checkpoint file found\")\n",
    "                return -1\n",
    "\n",
    "# 对图片进行预处理            \n",
    "def pre_pic(picName):\n",
    "    img = Image.open(picName)\n",
    "    reIm = img.resize((28,28), Image.ANTIALIAS)\n",
    "    im_arr = np.array(reIm.convert('L'))\n",
    "    threshold = 50\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            im_arr[i][j] = 255 - im_arr[i][j]\n",
    "             if (im_arr[i][j] < threshold):\n",
    "                im_arr[i][j] = 0\n",
    "            else: im_arr[i][j] = 255\n",
    "\n",
    "    nm_arr = im_arr.reshape([1, 784])\n",
    "    nm_arr = nm_arr.astype(np.float32)\n",
    "    img_ready = np.multiply(nm_arr, 1.0/255.0)\n",
    "\n",
    "    return img_ready\n",
    "\n",
    "def application():\n",
    "    testNum = input(\"input the number of test pictures:\")\n",
    "    for i in range(testNum):\n",
    "        testPic = raw_input(\"the path of test picture:\")\n",
    "        testPicArr = pre_pic(testPic)\n",
    "        preValue = restore_model(testPicArr)\n",
    "        print \"The prediction number is:\", preValue\n",
    "\n",
    "def main():\n",
    "    application()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 制作数据集(tfrecord格式)\n",
    "[Link1](https://blog.csdn.net/happyhorizion/article/details/77894055)\n",
    "\n",
    "> `tfrecord` 数据文件是一种将图像数据和标签统一存储的二进制文件，能更好的利用内存，在tensorflow中快速的复制，移动，读取，存储等\n",
    "\n",
    "> `tfrecord`文件包含了`tf.train.Example`协议缓冲区(protocol buffer，协议缓冲区包含了特征 Features)\n",
    "```python\n",
    "message Example {\n",
    "  Features features = 1;\n",
    "};\n",
    "message Features{\n",
    "  map<string,Feature> featrue = 1;\n",
    "};\n",
    "message Feature{\n",
    "  oneof kind{\n",
    "    BytesList bytes_list = 1;\n",
    "    FloatList float_list = 2;\n",
    "    Int64List int64_list = 3;\n",
    "  }\n",
    "};\n",
    "```\n",
    "\n",
    "> 写数据到TFRecords： 将数据填入到Example协议缓冲区(protocol buffer)，将协议缓冲区序列化为一个字符串， 并且通过`tf.python_io.TFRecordWriter` 写入到TFRecords文件。\n",
    "\n",
    "> 从TFRecords读数据： 使用`tf.TFRecordReader`的`tf.parse_single_example`解析器，这个操作可以将Example协议内存块(protocol buffer)解析为张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将数据保存为tfrecord格式\n",
    "[Link1](https://blog.csdn.net/happyhorizion/article/details/77894055)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建tfrecord文件\n",
    "```python\n",
    "tfrecords_filename = './tfrecords/train.tfrecords'\n",
    "# 创建tfrecord文件，准备写入\n",
    "writer = tf.python_io.TFRecordWriter(tfrecords_filename)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 写入数据到tfrecord\n",
    "\n",
    "从`tf.train.Example`的定义可知，`tfrecord`支持整型、浮点数和二进制三种格式，分别是\n",
    "```python\n",
    "* tf.train.Feature(int64_list = tf.train.Int64List(value=[int_scalar]))\n",
    "* tf.train.Feature(bytes_list = tf.train.BytesList(value=[array_string_or_byte]))\n",
    "* tf.train.Feature(bytes_list = tf.train.FloatList(value=[float_scalar]))\n",
    "```\n",
    "\n",
    "```python\n",
    "for i in range(100):\n",
    "    # 创建7*30，取值在0-255之间随机数组\n",
    "    img_raw = np.random.random_integers(0,255,size=(7,30)) \n",
    "    img_raw = img_raw.tostring()\n",
    "    example = tf.train.Example(features=tf.train.Features(\n",
    "        feature={\n",
    "            # 写入标签, 输入的必须是列表(list)\n",
    "            'label': tf.train.Feature(int64_list = tf.train.Int64List(value=[i])),  \n",
    "            # 写入图片数据\n",
    "            'img_raw':tf.train.Feature(bytes_list = tf.train.BytesList(value=[img_raw]))\n",
    "        }))\n",
    "    # 将数据序列化后写入到tfrecord文件中\n",
    "    writer.write(example.SerializeToString()) \n",
    "writer.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取tfrecord数据\n",
    "\n",
    "从`TFRecords`文件中读取数据， 首先需要用`tf.train.string_input_producer`生成一个解析队列，之后调用`tf.TFRecordReader`的`tf.parse_single_example`解析器\n",
    "\n",
    "```python\n",
    "tfrecords_filename = \"train.tfrecords\"\n",
    "filename_queue = tf.train.string_input_producer([tfrecords_filename], shuffle=True) \n",
    "reader = tf.TFRecordReader()\n",
    "_, serialized_example = reader.read(filename_queue)   # 返回文件名和文件\n",
    "\n",
    "# 取出包含image和label的feature对象\n",
    "features = tf.parse_single_example(serialized_example,\n",
    "                   features={\n",
    "                       'label': tf.FixedLenFeature([], tf.int64),\n",
    "                       'img_raw': tf.FixedLenFeature([], tf.string),\n",
    "                   }) \n",
    "image = tf.decode_raw(features['img_raw'],tf.int64)\n",
    "image = tf.reshape(image, [7,30])\n",
    "label = tf.cast(features['label'], tf.int64)\n",
    "\n",
    "with tf.Session() as sess: #开始一个会话\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    sess.run(init_op)\n",
    "    # 创建一个协调器，管理线程\n",
    "    coord=tf.train.Coordinator()\n",
    "    # 启动QueueRunner, 此时文件名队列已经进队\n",
    "    threads= tf.train.start_queue_runners(coord=coord)\n",
    "    for i in range(20):\n",
    "        example, l = sess.run([image,label])#在会话中取出image和label\n",
    "        img=Image.fromarray(example, 'RGB')#这里Image是之前提到的\n",
    "        img.save('./'+str(i)+'_''Label_'+str(l)+'.jpg')#存下图片\n",
    "        print(example, l)\n",
    "\n",
    "    # 请求线程结束\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [tfrecord 示例](https://blog.csdn.net/happyhorizion/article/details/77894055)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    " \n",
    "#=============================================================================#\n",
    "# write images and label in tfrecord file and read them out\n",
    "def encode_to_tfrecords(tfrecords_filename, data_num): \n",
    "    ''' write into tfrecord file '''\n",
    "    if os.path.exists(tfrecords_filename):\n",
    "        os.remove(tfrecords_filename)\n",
    "\n",
    "    # 创建.tfrecord文件，准备写入数据    \n",
    "    writer = tf.python_io.TFRecordWriter('./'+tfrecords_filename) \n",
    "\n",
    "    for i in range(data_num):\n",
    "        img_raw = np.random.randint(0,255,size=(56,56))\n",
    "        img_raw = img_raw.tostring()\n",
    "        example = tf.train.Example(features=tf.train.Features(\n",
    "            feature={\n",
    "                'label': tf.train.Feature(int64_list = tf.train.Int64List(value=[i])),     \n",
    "                'img_raw':tf.train.Feature(bytes_list = tf.train.BytesList(value=[img_raw]))\n",
    "            }))\n",
    "        writer.write(example.SerializeToString()) \n",
    "\n",
    "    writer.close()\n",
    "\n",
    " \n",
    "def decode_from_tfrecords(filename_queue, is_batch):\n",
    "    \n",
    "    reader = tf.TFRecordReader()\n",
    "    # 返回文件名和文件\n",
    "    _, serialized_example = reader.read(filename_queue)   \n",
    "    # 取出包含image和label的feature对象\n",
    "    features = tf.parse_single_example(serialized_example,\n",
    "                                    features={\n",
    "                                        'label': tf.FixedLenFeature([], tf.int64),\n",
    "                                        'img_raw' : tf.FixedLenFeature([], tf.string),\n",
    "                                    })  \n",
    "    image = tf.decode_raw(features['img_raw'],tf.int64)\n",
    "    image = tf.reshape(image, [56,56])\n",
    "    label = tf.cast(features['label'], tf.int64)\n",
    "\n",
    "    if is_batch:\n",
    "        batch_size = 3\n",
    "        num_threads = 3\n",
    "        min_after_dequeue = 10\n",
    "        # 需要注意: capacity >= min_after_dequeue + num_threads * batch_size\n",
    "        capacity = min_after_dequeue + num_threads * batch_size\n",
    "        image, label = tf.train.shuffle_batch([image, label],\n",
    "                                            batch_size=batch_size, \n",
    "                                            num_threads=num_threads, \n",
    "                                            capacity=capacity,\n",
    "                                            min_after_dequeue=min_after_dequeue)\n",
    "    return image, label\n",
    " \n",
    "#=============================================================================#\n",
    " \n",
    "if __name__=='__main__':\n",
    "    \n",
    "    # make train.tfrecord\n",
    "    train_filename = \"train.tfrecords\"\n",
    "    encode_to_tfrecords(train_filename,100)\n",
    "    \n",
    "    # make test.tfrecord\n",
    "    test_filename = 'test.tfrecords'\n",
    "    encode_to_tfrecords(test_filename,10)\n",
    "\n",
    "    filename_queue = tf.train.string_input_producer([train_filename],num_epochs=None)\n",
    "    train_image, train_label = decode_from_tfrecords(filename_queue, is_batch=True)\n",
    "\n",
    "    filename_queue = tf.train.string_input_producer([test_filename],num_epochs=None) \n",
    "    test_image, test_label = decode_from_tfrecords(filename_queue, is_batch=True)\n",
    "    with tf.Session() as sess: #开始一个会话\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        coord=tf.train.Coordinator()\n",
    "        threads= tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "        try:\n",
    "            # while not coord.should_stop():\n",
    "            for i in range(2):\n",
    "                example, l = sess.run([train_image,train_label])#在会话中取出image和label\n",
    "                print('train:')\n",
    "                print(example, l) \n",
    "                texample, tl = sess.run([test_image, test_label])\n",
    "                print('test:')\n",
    "                print(texample,tl)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Done reading')\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN(Convolutional Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional (卷积)\n",
    "> 卷积操作可以认为是一种有效提取图像特征的方法\n",
    "\n",
    "> 一般会用一个正方形卷积核，遍历图片上的每一个点。图片区域内，相对应的每一个像素值，乘以卷积核内相对应点的权重，求和，再加上偏置\n",
    "\n",
    "> VALID 模式(不全零填充)\n",
    ">> 输出图片边长(向上取整) = (输入图片边长 - 卷积核长 + 1) / 步长\n",
    "\n",
    "> SAME 模式(全零填充)\n",
    ">> 输出图片边长(向上取整) = 输入图片边长 / 步长"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling (池化)\n",
    "> 池化操作用于减少特征数量\n",
    "\n",
    "> 最大值池化可提取图片纹理\n",
    "\n",
    "> 均值池化可保留背景特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout (舍弃)\n",
    "> 在神经网络的训练过程中，将一部分神经元按照一定概率从神经网络中暂时舍弃；在使用的过程中，被舍弃的神经元恢复链接\n",
    "```pyhton\n",
    "tf.nn.dropout(上层输出, 暂时舍弃的概率)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lenet-5 示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义神经网络模型(lenet5_forward.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "\n",
    "\"\"\"\n",
    "    filename: lenet5_forward.py\n",
    "    定义前向神经网络模型\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "CONV1_SIZE = 5\n",
    "CONV1_KERNEL_NUM = 32\n",
    "CONV2_SIZE = 5\n",
    "CONV2_KERNEL_NUM = 64\n",
    "FC_SIZE = 512\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "def get_weight(shape, regularizer):\n",
    "    w = tf.Variable(tf.truncated_normal(shape,stddev=0.1))\n",
    "    if regularizer != None: tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(w)) \n",
    "    return w\n",
    "\n",
    "def get_bias(shape): \n",
    "    b = tf.Variable(tf.zeros(shape))  \n",
    "    return b\n",
    "\n",
    "def conv2d(x,w):  \n",
    "    return tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):  \n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') \n",
    "\n",
    "def forward(x, train, regularizer):\n",
    "    conv1_w = get_weight([CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_KERNEL_NUM], regularizer) \n",
    "    conv1_b = get_bias([CONV1_KERNEL_NUM]) \n",
    "    conv1 = conv2d(x, conv1_w) \n",
    "    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_b)) \n",
    "    pool1 = max_pool_2x2(relu1) \n",
    "\n",
    "    conv2_w = get_weight([CONV2_SIZE, CONV2_SIZE, CONV1_KERNEL_NUM, CONV2_KERNEL_NUM],regularizer) \n",
    "    conv2_b = get_bias([CONV2_KERNEL_NUM])\n",
    "    conv2 = conv2d(pool1, conv2_w) \n",
    "    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_b))\n",
    "    pool2 = max_pool_2x2(relu2)\n",
    "\n",
    "    pool_shape = pool2.get_shape().as_list() \n",
    "    nodes = pool_shape[1] * pool_shape[2] * pool_shape[3] \n",
    "    reshaped = tf.reshape(pool2, [pool_shape[0], nodes]) \n",
    "\n",
    "    fc1_w = get_weight([nodes, FC_SIZE], regularizer) \n",
    "    fc1_b = get_bias([FC_SIZE]) \n",
    "    fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_w) + fc1_b) \n",
    "    if train: fc1 = tf.nn.dropout(fc1, 0.5)\n",
    "\n",
    "    fc2_w = get_weight([FC_SIZE, OUTPUT_NODE], regularizer)\n",
    "    fc2_b = get_bias([OUTPUT_NODE])\n",
    "    y = tf.matmul(fc1, fc2_w) + fc2_b\n",
    "    return y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练神经网络(lenet5_backward.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "\n",
    "\"\"\"\n",
    "    filename: lenet5_backward.py\n",
    "    训练神经网络模型\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import lenet5_forward\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE_BASE =  0.005 \n",
    "LEARNING_RATE_DECAY = 0.99 \n",
    "REGULARIZER = 0.0001 \n",
    "STEPS = 50000 \n",
    "MOVING_AVERAGE_DECAY = 0.99 \n",
    "MODEL_SAVE_PATH=\"./model/\" \n",
    "MODEL_NAME=\"mnist_model\" \n",
    "\n",
    "def backward(mnist):\n",
    "    x = tf.placeholder(tf.float32,[\n",
    "    BATCH_SIZE,\n",
    "    lenet5_forward.IMAGE_SIZE,\n",
    "    lenet5_forward.IMAGE_SIZE,\n",
    "    lenet5_forward.NUM_CHANNELS]) \n",
    "    y_ = tf.placeholder(tf.float32, [None, lenet5_forward.OUTPUT_NODE])\n",
    "    y = mnist_lenet5_forward.forward(x,True, REGULARIZER) \n",
    "    global_step = tf.Variable(0, trainable=False) \n",
    "\n",
    "    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    cem = tf.reduce_mean(ce) \n",
    "    loss = cem + tf.add_n(tf.get_collection('losses')) \n",
    "\n",
    "    learning_rate = tf.train.exponential_decay( \n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples / BATCH_SIZE, \n",
    "        LEARNING_RATE_DECAY,\n",
    "        staircase=True) \n",
    "    \n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    ema_op = ema.apply(tf.trainable_variables())\n",
    "    with tf.control_dependencies([train_step, ema_op]): \n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    saver = tf.train.Saver() \n",
    "\n",
    "    with tf.Session() as sess: \n",
    "        init_op = tf.global_variables_initializer() \n",
    "        sess.run(init_op) \n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH) \n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path) \n",
    "\n",
    "        for i in range(STEPS):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE) \n",
    "            reshaped_xs = np.reshape(xs,(  \n",
    "            BATCH_SIZE,\n",
    "            mnist_lenet5_forward.IMAGE_SIZE,\n",
    "            mnist_lenet5_forward.IMAGE_SIZE,\n",
    "            mnist_lenet5_forward.NUM_CHANNELS))\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: reshaped_xs, y_: ys}) \n",
    "            if i % 100 == 0: \n",
    "                print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))\n",
    "                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n",
    "\n",
    "def main():\n",
    "    mnist = input_data.read_data_sets(\"./data/\", one_hot=True) \n",
    "    backward(mnist)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试神经网络模型(lenet5_test.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "\n",
    "\"\"\"\n",
    "    filename: lenet5_test.py\n",
    "    测试神经网络模型\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import lenet5_forward\n",
    "import lenet5_backward\n",
    "import numpy as np\n",
    "\n",
    "TEST_INTERVAL_SECS = 5\n",
    "\n",
    "def test(mnist):\n",
    "    with tf.Graph().as_default() as g: \n",
    "        x = tf.placeholder(tf.float32,[\n",
    "            mnist.test.num_examples,\n",
    "            lenet5_forward.IMAGE_SIZE,\n",
    "            lenet5_forward.IMAGE_SIZE,\n",
    "            lenet5_forward.NUM_CHANNELS]) \n",
    "        y_ = tf.placeholder(tf.float32, [None, lenet5_forward.OUTPUT_NODE])\n",
    "        y = mnist_lenet5_forward.forward(x,False,None)\n",
    "\n",
    "        ema = tf.train.ExponentialMovingAverage(lenet5_backward.MOVING_AVERAGE_DECAY)\n",
    "        ema_restore = ema.variables_to_restore()\n",
    "        saver = tf.train.Saver(ema_restore)\n",
    "        \n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) \n",
    "\n",
    "        while True:\n",
    "            with tf.Session() as sess:\n",
    "                ckpt = tf.train.get_checkpoint_state(lenet5_backward.MODEL_SAVE_PATH)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                    \n",
    "                    global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1] \n",
    "                    reshaped_x = np.reshape(mnist.test.images,(\n",
    "                    mnist.test.num_examples,\n",
    "                    mnist_lenet5_forward.IMAGE_SIZE,\n",
    "                    mnist_lenet5_forward.IMAGE_SIZE,\n",
    "                    mnist_lenet5_forward.NUM_CHANNELS))\n",
    "                    accuracy_score = sess.run(accuracy, feed_dict={x:reshaped_x,y_:mnist.test.labels}) \n",
    "                    print(\"After %s training step(s), test accuracy = %g\" % (global_step, accuracy_score))\n",
    "                else:\n",
    "                    print('No checkpoint file found')\n",
    "                    return\n",
    "            time.sleep(TEST_INTERVAL_SECS) \n",
    "\n",
    "def main():\n",
    "    mnist = input_data.read_data_sets(\"./data/\", one_hot=True)\n",
    "    test(mnist)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "675px",
    "left": "56px",
    "top": "95px",
    "width": "315px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
