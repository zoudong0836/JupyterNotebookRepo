{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Tensorboard使用流程\" data-toc-modified-id=\"Tensorboard使用流程-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Tensorboard使用流程</a></span></li><li><span><a href=\"#Tensorboard的数据形式\" data-toc-modified-id=\"Tensorboard的数据形式-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Tensorboard的数据形式</a></span></li><li><span><a href=\"#通过命名空间美化计算图\" data-toc-modified-id=\"通过命名空间美化计算图-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>通过命名空间美化计算图</a></span></li><li><span><a href=\"#tf.summary.merge_all\" data-toc-modified-id=\"tf.summary.merge_all-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>tf.summary.merge_all</a></span></li><li><span><a href=\"#tf.summary.FileWriter\" data-toc-modified-id=\"tf.summary.FileWriter-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>tf.summary.FileWriter</a></span></li><li><span><a href=\"#完整示例-(一)\" data-toc-modified-id=\"完整示例-(一)-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>完整示例 (一)</a></span></li><li><span><a href=\"#完整示例-(二)\" data-toc-modified-id=\"完整示例-(二)-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>完整示例 (二)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard使用流程\n",
    "[Link1](http://blog.csdn.net/mzpmzk/article/details/77914941)\n",
    "\n",
    "* 添加记录节点 `tf.summary.scalar/ tf.summary.image / tf.summary.histogram`\n",
    "* 汇总记录节点 `merged = tf.summary.merge_all()`\n",
    "* 运行汇总节点 `summary = ses.run(merged) # 得到汇总结果` \n",
    "* 日志书写器实例化 `summary_writer = tf.summary.FileWriter(logdir, graph=sess.graph) # 实例化的同时传入graph将当前计算图写入日子`\n",
    "* 调用日志书写器对象 `summary_writer.add_summary(summary, global_step=i)`\n",
    "* 关闭日志书写器对象 `summary_writer.close()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard的数据形式\n",
    "***\n",
    "\n",
    "* 标量Scalars            (使用`tf.summary.scalar`记录)\n",
    "* 图片Images\n",
    "* 音频Audio\n",
    "* 计算图Graph           （使用`tf.summary.image`记录）\n",
    "* 数据分布Distribution  （使用`tf.summary.distribution`记录）\n",
    "* 直方图Histograms      （使用`tf.summary.histogram`记录）\n",
    "* 嵌入向量Embeddings\n",
    "\n",
    "```python\n",
    "# 标量\n",
    "tf.summary.scalar('loss', loss)\n",
    "# 备注： 只能展示一个实数值, 也就是一般用来显示准确度、损失等，不能用来展示训练好的权重矩阵或向量\n",
    "# 参数：tf.summary.scalar(name, tensor, collections=None, family=None)\n",
    "* name:   A name for the generated node. Will also serve as the series name in TensorBoard.\n",
    "* tensor:  A real numeric Tensor containing a single value.\n",
    "* collections:  Optional list of graph collections keys. The new summary op is added to these collections. Defaults to `[GraphKeys.SUMMARIES]`.\n",
    "* family:   Optional; if provided, used as the prefix of the summary tag name, which controls the tab name used for display on Tensorboard.\n",
    "\n",
    "# 直方图\n",
    "tf.summary.histogram('bias', bias)\n",
    "# 备注： 只能展示tensor的分布，并不能表示tensor的值\n",
    "# 参数： tf.sumary.histogram(name, values, collections=None, family=None):\n",
    "* name: string，表示在tensorboard显示的名称\n",
    "* values: tensor，一般是多维tensor\n",
    "* collections: 表示将这个类加入到哪个集合中\n",
    "* family: 类似命名前缀。一般只要在定义图的时候进行了命名空间定义,如name_scope、variable_scope，就不需要这family了\n",
    "\n",
    "# 图像\n",
    "tf.summary.image('img', tf.reshape(images, [-1, 28, 28, 1]), 10)\n",
    "# 备注: reshape里面的四个参数分别代表[图像数, 每幅图的高度, 每幅图的宽度, 每幅图的通道数]，-1表示根据实际数据（在这里是images）进行动态计算; 第三个参数\"10\"表示最多展示十幅图\n",
    "\n",
    "# 音频\n",
    "tf.summary.audio('audio', audio, sampling_frequency)\n",
    "# 备注: audio是一个三维或者二维tensor，含义是[音频数, 每个音频的帧数, 每个音频的通道数]或者[音频数, 每个音频的帧数]。sampling_frequency从名字就可以看出来了，就是音频的采样率\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过命名空间美化计算图\n",
    "\n",
    "* 使用命名空间使可视化效果图更有层次性，使得神经网络的整体结构不会被过多的细节所淹没\n",
    "* 同一个命名空间下的所有节点会被缩略成一个节点，只有顶层命名空间中的节点才会被显示在 TensorBoard 可视化效果图上\n",
    "* 可通过`tf.name_scope()`或者`tf.variable_scope()`来实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tf.variable_scope可以让变量有相同的命名，包括tf.get_variable得到的变量，还有tf.Variable的变量\n",
    "tf.name_scope可以让变量有相同的命名，只是限于tf.Variable的变量 \n",
    "\n",
    "可以看出，对于tf.Variable()函数，两者的使用情况都一样；\n",
    "而tf.get_variable()函数，它不受name_scope约束，已经声明过的变量就不能再声明了\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.variable_scope('V1'):\n",
    "    a1 = tf.get_variable(name='a1', shape=[1], initializer=tf.constant_initializer(1))\n",
    "    a2 = tf.Variable(tf.random_normal(shape=[2,3], mean=0, stddev=1), name='a2')\n",
    "with tf.variable_scope('V2'):\n",
    "    a3 = tf.get_variable(name='a1', shape=[1], initializer=tf.constant_initializer(1))\n",
    "    a4 = tf.Variable(tf.random_normal(shape=[2,3], mean=0, stddev=1), name='a2')\n",
    "  \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print a1.name       # V1/a1:0\n",
    "    print a2.name       # V1/a2:0\n",
    "    print a3.name       # V2/a1:0\n",
    "    print a4.name       # V2/a1:0\n",
    "    \n",
    "    \n",
    "with tf.name_scope('V1'):\n",
    "    # a1 = tf.get_variable(name='a1', shape=[1], initializer=tf.constant_initializer(1))\n",
    "    a2 = tf.Variable(tf.random_normal(shape=[2,3], mean=0, stddev=1), name='a2')\n",
    "with tf.name_scope('V2'):\n",
    "    # a3 = tf.get_variable(name='a1', shape=[1], initializer=tf.constant_initializer(1))\n",
    "    a4 = tf.Variable(tf.random_normal(shape=[2,3], mean=0, stddev=1), name='a2')\n",
    "  \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # print a1.name    # 报错: Variable a1 already exists, disallowed. \n",
    "    print a2.name      # V1/a2:0\n",
    "    # print a3.name    # 报错: Variable a1 already exists, disallowed. \n",
    "    print a4.name      # V2/a2:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.summary.merge_all\n",
    "***\n",
    "\n",
    "tensorflow 中的 operations 并不会立即执行计算, 除非显示调用run去执行该operation, 或者它被其他的需要run的operation所依赖。但是一些常用的summary operations其实并不被其他节点依赖，因此，需要特地去运行所有的summary节点。但是，一份程序下来可能有超多这样的summary 节点，要手动一个一个去启动自然是及其繁琐的，可以使用`tf.summary.merge_all`去将所有summary节点合并成一个节点，只要运行这个节点，就能产生所有之前设置的summary data。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.summary.FileWriter\n",
    "***\n",
    "\n",
    "使用`tf.summary.FileWriter`可以将运行后输出的数据都保存到本地磁盘中\n",
    "\n",
    "参数说明：\n",
    "```python\n",
    "def __init__(self,\n",
    "           logdir,\n",
    "           graph=None,\n",
    "           max_queue=10,\n",
    "           flush_secs=120,\n",
    "           graph_def=None,\n",
    "           filename_suffix=None,\n",
    "           session=None):\n",
    "        \n",
    "Args:\n",
    "  logdir:   A string. Directory where event file will be written.\n",
    "  graph:    A `Graph` object, such as `sess.graph`.\n",
    "  max_queue:   Integer. Size of the queue for pending events and summaries.\n",
    "  flush_secs:   Number. How often, in seconds, to flush the pending events and summaries to disk.\n",
    "  graph_def:  DEPRECATED: Use the `graph` argument instead.\n",
    "  filename_suffix:  A string. Every event files name is suffixed with suffix\n",
    "  session:  A `tf.Session` object. See details above.        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整示例 (一)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 10\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "logs_path = 'tensorflow_logs'\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 20 # 1st layer number of features\n",
    "n_hidden_2 = 40 # 2nd layer number of features\n",
    "n_input = 784   # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph Input\n",
    "# mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "# 0-9 digits recognition => 10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "# 使用tf.summary.scalar记录标量\n",
    "# 使用tf.summary.histogram记录数据的直方图\n",
    "# 使用tf.summary.distribution记录数据的分布图\n",
    "# 使用tf.summary.image记录图像数据\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Create a summary to visualize the first layer ReLU activation\n",
    "    tf.summary.histogram(\"relu1\", layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Create another summary to visualize the second layer ReLU activation\n",
    "    tf.summary.histogram(\"relu2\", layer_2)\n",
    "    # Output layer\n",
    "    out_layer = tf.add(tf.matmul(layer_2, weights['w3']), biases['b3'])\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'w1': tf.Variable(tf.random_normal([n_input, n_hidden_1]), name='W1'),\n",
    "    'w2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]), name='W2'),\n",
    "    'w3': tf.Variable(tf.random_normal([n_hidden_2, n_classes]), name='W3')\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1]), name='b1'),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2]), name='b2'),\n",
    "    'b3': tf.Variable(tf.random_normal([n_classes]), name='b3')\n",
    "}\n",
    "\n",
    "# Encapsulating all ops into scopes, making Tensorboard's Graph\n",
    "# Visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Build model\n",
    "    pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "with tf.name_scope('Loss'):\n",
    "    # Softmax Cross entropy (cost function)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    # Op to calculate every variable gradient\n",
    "    grads = tf.gradients(loss, tf.trainable_variables())\n",
    "    grads = list(zip(grads, tf.trainable_variables()))\n",
    "    # Op to update all variables according to their gradient\n",
    "    apply_grads = optimizer.apply_gradients(grads_and_vars=grads)\n",
    "\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"accuracy\", acc)\n",
    "# Create summaries to visualize weights\n",
    "for var in tf.trainable_variables():\n",
    "    tf.summary.histogram(var.name, var)\n",
    "# Summarize all gradients\n",
    "for grad, var in grads:\n",
    "    tf.summary.histogram(var.name + '/gradient', grad)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            _, c, summary = sess.run([apply_grads, loss, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    # Calculate accuracy\n",
    "    print(\"Accuracy:\", acc.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "\n",
    "    print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=/tmp/tensorflow_logs \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整示例 (二)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "# 准备训练数据，假设其分布大致符合 y = 1.2x + 0.0\n",
    "n_train_samples = 200\n",
    "X_train = np.linspace(-5, 5, n_train_samples)\n",
    "Y_train = 1.2*X_train + np.random.uniform(-1.0, 1.0, n_train_samples)  # 加一点随机扰动\n",
    "\n",
    "\n",
    "# 准备验证数据，用于验证模型的好坏\n",
    "n_test_samples = 50\n",
    "X_test = np.linspace(-5, 5, n_test_samples)\n",
    "Y_test = 1.2*X_test\n",
    "\n",
    "\n",
    "# 参数学习算法相关变量设置\n",
    "learning_rate = 0.01\n",
    "batch_size = 20\n",
    "summary_dir = 'logs'\n",
    "\n",
    "print '~~~~~~~~~~开始设计计算图~~~~~~~~'\n",
    "\n",
    "# 使用 placeholder 将训练数据/验证数据送入网络进行训练/验证\n",
    "# shape=None 表示形状由送入的张量的形状来确定\n",
    "with tf.name_scope('Input'):\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=None, name='X')\n",
    "    Y = tf.placeholder(dtype=tf.float32, shape=None, name='Y')\n",
    "\n",
    "\n",
    "# 决策函数(参数初始化)\n",
    "with tf.name_scope('Inference'):\n",
    "    W = tf.Variable(initial_value=tf.truncated_normal(shape=[1]), name='weight')\n",
    "    b = tf.Variable(initial_value=tf.truncated_normal(shape=[1]), name='bias')\n",
    "    Y_pred = tf.multiply(X, W) + b\n",
    "\n",
    "\n",
    "# 损失函数(MSE)\n",
    "with tf.name_scope('Loss'):\n",
    "    loss = tf.reduce_mean(tf.square(Y_pred - Y), name='loss')\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "\n",
    "# 参数学习算法(Mini-batch SGD)\n",
    "with tf.name_scope('Optimization'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "# 初始化所有变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# 汇总记录节点\n",
    "merge = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "# 开启会话，进行训练\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    summary_writer = tf.summary.FileWriter(logdir=summary_dir, graph=sess.graph)\n",
    "\n",
    "    for i in range(201):\n",
    "        j = np.random.randint(0, 10)  # 总共200训练数据，分十份[0, 9]\n",
    "        X_batch = X_train[batch_size*j: batch_size*(j+1)]\n",
    "        Y_batch = Y_train[batch_size*j: batch_size*(j+1)]\n",
    "\n",
    "        _, summary, train_loss, W_pred, b_pred = sess.run([optimizer, merge, loss, W, b], feed_dict={X: X_batch, Y: Y_batch})\n",
    "        test_loss = sess.run(loss, feed_dict={X: X_test, Y: Y_test})\n",
    "\n",
    "        # 将所有日志写入文件\n",
    "        summary_writer.add_summary(summary, global_step=i)\n",
    "        print('step:{}, losses:{}, test_loss:{}, w_pred:{}, b_pred:{}'.format(i, train_loss, test_loss, W_pred[0], b_pred[0]))\n",
    "\n",
    "        if i == 200:\n",
    "            # plot the results\n",
    "            plt.plot(X_train, Y_train, 'bo', label='Train data')\n",
    "            plt.plot(X_test, Y_test, 'gx', label='Test data')\n",
    "            plt.plot(X_train, X_train * W_pred + b_pred, 'r', label='Predicted data')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    summary_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
