{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#循环神经网络-(recurrent-neural-network,-RNN)\" data-toc-modified-id=\"循环神经网络-(recurrent-neural-network,-RNN)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>循环神经网络 (recurrent neural network, RNN)</a></span></li><li><span><a href=\"#长短时记忆网络-(LTSM)\" data-toc-modified-id=\"长短时记忆网络-(LTSM)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>长短时记忆网络 (LTSM)</a></span></li><li><span><a href=\"#循环神经网络的变种\" data-toc-modified-id=\"循环神经网络的变种-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>循环神经网络的变种</a></span><ul class=\"toc-item\"><li><span><a href=\"#双向循环神经网络-(bidirectional-RNN)\" data-toc-modified-id=\"双向循环神经网络-(bidirectional-RNN)-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>双向循环神经网络 (bidirectional RNN)</a></span></li><li><span><a href=\"#深层循环神经网络-(deepRNN)\" data-toc-modified-id=\"深层循环神经网络-(deepRNN)-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>深层循环神经网络 (deepRNN)</a></span></li><li><span><a href=\"#循环神经网络的dropout\" data-toc-modified-id=\"循环神经网络的dropout-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>循环神经网络的dropout</a></span></li></ul></li><li><span><a href=\"#利用RNN训练手写数字MNIST数据集\" data-toc-modified-id=\"利用RNN训练手写数字MNIST数据集-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>利用RNN训练手写数字MNIST数据集</a></span><ul class=\"toc-item\"><li><span><a href=\"#tf.nn.dynamic_rnn-函数说明\" data-toc-modified-id=\"tf.nn.dynamic_rnn-函数说明-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>tf.nn.dynamic_rnn 函数说明</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环神经网络 (recurrent neural network, RNN)\n",
    "[Link1](https://www.imooc.com/article/23821)\n",
    "\n",
    "\n",
    "> 循环神经网络的主要用途是处理和预测`序列数据`。循环神经网络的隐藏层之间的结点是有连接的，隐藏层的输入不仅包括输入层的输出，还包括上一时刻隐藏层的输出。\n",
    "\n",
    "> 循环神经网络会对于每一个时刻的输入结合当前模型的状态给出一个输出。循环神经网络在每一个时刻会有一个输入X(t)，然后根据循环神经网络当前的状态A(t)提供一个输出H(t)；而循环神经网络当前的状态A(t)是根据上一时刻的状态A(t-1)和当前的输入X(t)共同决定的。\n",
    "\n",
    "> 对于一个序列数据，可以将这个序列上不同时刻的数据依次传入循环神经网络的输入层，而输出可以是对序列中下一个时刻的预测，也可以是对当前时刻信息的处理结果。\n",
    "\n",
    "> 在循环神经网络中，循环体网络结构中的参数`在不同时刻是共享的`。\n",
    "\n",
    "> 循环神经网络的总损失为所有时刻(或者部分时刻)上的损失函数的总和。\n",
    "\n",
    "![RNN前向传播计算示意图](illustration/RNN前向传播计算示意图.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 长短时记忆网络 (LTSM)\n",
    "\n",
    "> 循环神经网络工作的关键点是使用历史的信息来帮助当前的决策；但同时也带来了更大的技术挑战 -- 长期依赖(long-term dependencies)\n",
    "\n",
    "> LSTM的提出就是为了解决长期依赖问题。LSTM是一种拥有三个\"门\"结构的特殊网络结构。(所谓\"门\"的结构就是一个使用sigmoid神经网络和和一个按位做乘法的操作，这两个操作合在一起就是一个\"门\"结构，之所以该结构叫做\"门\"是因为使用sigmoid作为激活函数的全连接神经网络层会输出一个0到1之间的数值，描述当前输入有多少信息量可以通过这个结构)\n",
    "\n",
    "> \"遗忘门\"会根据当前的输入`x(t)`, 上一时刻状态`c(t-1)`和上一时刻输出`h(t-1)`共同决定哪一部分记忆需要被遗忘\n",
    "\n",
    "> \"输入门\"会根据`x(t)`、`c(t-1)`和`h(t-1)`决定哪些部分将进入当前时刻的状态`c(t)`\n",
    "\n",
    "> \"输出门\"会根据最新的状态`c(t)`、上一时刻的输出`h(t-1)`和当前的输入`x(t)`来决定该时刻的输出`h(t)`\n",
    "\n",
    "\n",
    "![LSTM单元结构示意图](illustration/LSTM单元结构示意图.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个LSTM结构, 并指定隐藏层的大小\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_hidden_size)\n",
    "'''\n",
    "__init__(\n",
    "    num_units,\n",
    "    forget_bias=1.0,\n",
    "    state_is_tuple=True,\n",
    "    activation=None,\n",
    "    reuse=None,\n",
    "    name=None,\n",
    "    dtype=None\n",
    ")\n",
    "'''\n",
    "\n",
    "# 将LSTM中的状态初始化为全0数组. 和其他神经网络类似, 在优化神经网络时, \n",
    "# 每次也会使用一个batch的训练样本, 以下代码中, batch_size给出了一个batch大小\n",
    "# BasicLSTMCell类提供了zero_state函数来生成全0的初始状态\n",
    "state = lstm.zero_state(batch_size, tf.float32)\n",
    "'''\n",
    "zero_state(batch_size, dtype)\n",
    "Args:\n",
    "    batch_size:  int, float, or unit Tensor representing the batch size.\n",
    "    dtype:  the data type to use for the state.\n",
    "Return:\n",
    "    zero-filled state tensor(s).    \n",
    "'''\n",
    "\n",
    "# 虽然理论上循环神经网络可以处理任意长度的序列, 但是在训练时为了避免梯度消散的问题,\n",
    "# 通常会规定一个最大的序列长度, 以下代码中, 用 num_steps 来表示这个长度\n",
    "for i in range(num_steps):\n",
    "    # 在第一个时刻声明LSTM结构中使用的变量\n",
    "    # 在之后的时刻都需要复用之前定义好的变量\n",
    "    if i > 0:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    # 每一步处理时间序列中的一个时刻。将当前输入(current_input)和前一时刻状态(state)\n",
    "    # 传入定义的LSTM结构可以得到当前LSTM结构的输出lstm_output和更新后的状态state\n",
    "    lstm_output, state = lstm(current_input, state)\n",
    "    # 将当前时刻LSTM结构的输出传入一个全连接层得到最后的输出\n",
    "    final_output = fully_connected(lstm_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环神经网络的变种"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 双向循环神经网络 (bidirectional RNN)\n",
    "\n",
    "双向循环神经网络是由两个循环神经网络上下叠加在一起组成的。输出由这两个循环神经网络的状态共同决定。  \n",
    "![双向循环神经网络结构示意图](illustration/双向循环神经网络结构示意图.png)  \n",
    "\n",
    "> 双向循环神经网络的主体结构就是两个单向循环神经网络的结合。在每一个时刻`t`, 输入会同时提供给这两个方向相反的循环神经网络, 而输出则是由这两个单向循环神经网络共同决定。双向循环神经网络的前向传播过程和单向的循环神经网络十分类似"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深层循环神经网络 (deepRNN)\n",
    "深层循环神经网络在每一个时刻上将循环体结构复制了多次，每一层的循环体中参数是一致的，而不同层中的参数可以不同   \n",
    "![深层循环神经网络结构示意图](illustration/深层循环神经网络结构示意图.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 循环神经网络的dropout\n",
    "在循环神经网络中使用`dropout`可以使网络更加健壮；区别于卷积神经网络只在最后的全连接层中使用`dropout`, 循环神经网络一般只在不同层循环体结构之间使用`dropout`, 而不在同一层的循环体结构之间使用。也就是说从时刻`t-1`传递到时刻`t`时，循环神经网络不会进行状态的`dropout`；而在同一时刻`t`中，不同层循环体之间会使用`dropout`  \n",
    "![深层循环神经网络使用dropout示意图](illustration/深层循环神经网络使用dropout示意图.png)\n",
    "\n",
    "> 图中实线箭头表示不使用`dropout`，虚线箭头表示使用`dropout`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用RNN训练手写数字MNIST数据集\n",
    "[Link1](https://blog.csdn.net/omnispace/article/details/78415100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.set_random_seed(1)   # set random seed\n",
    " \n",
    "# 导入数据\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    " \n",
    "# hyperparameters\n",
    "lr = 0.001                         # learning rate\n",
    "training_iters = 100000     # train step 上限\n",
    "batch_size = 128            \n",
    "n_inputs = 28                  # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28                   # time steps\n",
    "n_hidden_units = 128       # neurons in hidden layer\n",
    "n_classes = 10                # MNIST classes (0-9 digits)\n",
    "\n",
    "# x y placeholder\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    " \n",
    "# 对 weights biases 初始值的定义\n",
    "weights = {\n",
    "    # shape (28, 128)\n",
    "    'in': tf.Variable(tf.random_normal([n_inputs, n_hidden_units])),\n",
    "    # shape (128, 10)\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_units, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    # shape (128, )\n",
    "    'in': tf.Variable(tf.constant(0.1, shape=[n_hidden_units, ])),\n",
    "    # shape (10, )\n",
    "    'out': tf.Variable(tf.constant(0.1, shape=[n_classes, ]))\n",
    "}\n",
    "\n",
    "'''\n",
    "  RNN 主体总共有3部分组成(input_layer[输入隐藏层],  rnn_cell[rnn网络层],  output_layer[输出隐藏层])\n",
    "'''\n",
    "def RNN(X, weights, biases):\n",
    "    # 定义 input_layer 层...\n",
    "    # 原始的 X 是 3 维数据, 我们需要把它变成 2 维数据才能使用 weights 的矩阵乘法\n",
    "    # X ==> (128 batches * 28 steps, 28 inputs)\n",
    "    X = tf.reshape(X, [-1, n_inputs])\n",
    "    # X_in = W*X + b\n",
    "    X_in = tf.matmul(X, weights['in']) + biases['in']\n",
    "    # X_in ==> (128 batches, 28 steps, 128 hidden) 换回3维\n",
    "    X_in = tf.reshape(X_in, [-1, n_steps, n_hidden_units])\n",
    "\n",
    "    # 定义 rnn_cell 层...\n",
    "    # 使用 basic LSTM Cell.\n",
    "    # 对于 lstm 来说, state可被分为(c_state, h_state).\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_units, forget_bias=1.0, state_is_tuple=True)\n",
    "    # 初始化全零 state\n",
    "    init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32) \n",
    "    # 如果 inputs 为 (batches, steps, inputs) ==> time_major=False;\n",
    "    # 如果 inputs 为 (steps, batches, inputs) ==> time_major=True;\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, X_in, initial_state=init_state, time_major=False)\n",
    "\n",
    "    # 定义 output_layer 层...\n",
    "    # 由于该例的特殊性, 有两种方法来计算 output_layer 输出\n",
    "    # 方式一: 直接调用final_state 中的 h_state ( 通过final_state[1]获取) 来进行运算:\n",
    "    results = tf.matmul(final_state[1], weights['out']) + biases['out']  \n",
    "    # 方式二: 调用最后一个 outputs (在这个例子中, 和上面的final_state[1]是一样的):\n",
    "    # 由于time_major=False,  故outputs 的形状为 [batch_size, max_time, cell.output_size]\n",
    "    # 把 outputs 变成 列表 [(batch, outputs)..] * steps\n",
    "    outputs = tf.unstack(tf.transpose(outputs, [1,0,2]))\n",
    "    results = tf.matmul(outputs[-1], weights['out']) + biases['out']    #选取最后一个 output\n",
    "\n",
    "return results\n",
    "\n",
    "# 定义 cost 和 train_op:\n",
    "pred = RNN(x, weights, biases)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "train_op = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "\n",
    "# 训练 RNN\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "init = tf.global_variables_initializer()\n",
    " \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 0\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        batch_xs = batch_xs.reshape([batch_size, n_steps, n_inputs])\n",
    "        sess.run([train_op], feed_dict={\n",
    "            x: batch_xs,\n",
    "            y: batch_ys,\n",
    "        })\n",
    "        if step % 20 == 0:\n",
    "            print(sess.run(accuracy, feed_dict={\n",
    "            x: batch_xs,\n",
    "            y: batch_ys,\n",
    "        }))\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.nn.dynamic_rnn 函数说明\n",
    "\n",
    "```python\n",
    "tf.nn.dynamic_rnn(\n",
    "    cell,\n",
    "    inputs,\n",
    "    sequence_length=None,\n",
    "    initial_state=None,\n",
    "    dtype=None,\n",
    "    parallel_iterations=None,\n",
    "    swap_memory=False,\n",
    "    time_major=False,\n",
    "    scope=None\n",
    ")\n",
    "```\n",
    "\n",
    "Returns:   A pair (outputs, state)\n",
    "[Link1](https://blog.csdn.net/u010960155/article/details/81707498)\n",
    "\n",
    "outputs 是一个tensor  \n",
    "> 如果time_major==True，outputs形状为 [max_time, batch_size, cell.output_size]（要求rnn输入与rnn输出形状保持一致）     \n",
    "\n",
    "> 如果time_major==False(默认)，outputs形状为 [batch_size, max_time, cell.output_size]\n",
    "   \n",
    "\n",
    "state 是一个tensor  \n",
    "> 如果cell为LSTM, state形状为[2，batch_size, cell.output_size]; 每个cell会有两个输出: `C(t)` 和 `h(t)`，上面这个图是输出`C(t)`，代表哪些信息应该被记住哪些应该被遗忘；下面这个图是输出`h(t)`，代表这个cell的最终输出，LSTM的state是由`C(t)` 和 `h(t)` 组成的\n",
    "\n",
    "> 如果cell为GRU, state形状为[batch_size, cell.output_size];  state就只有一个了，原因是GRU将`C(t)` 和 `h(t)` 进行了简化，将其合并成了`h(t)`，如下图所示，GRU将遗忘门和输入门合并成了更新门，另外cell不在有细胞状态cell state，只有hidden state\n",
    "\n",
    "output 与 state 的关系\n",
    "> 如果cell为LSTM，那 state是个tuple，分别代表`C(t)` 和 `h(t)` ，其中 `h(t)` 与outputs中的对应的最后一个时刻的输出相等，假设state形状为[2，batch_size, cell.output_size]，outputs形状为 [batch_size, max_time, cell.output_size]，那么`state[1, batch_size, : ]` == `outputs[batch_size, -1, : ]`\n",
    "\n",
    "> 如果cell为GRU，同上，state其实就是 `h(t)`，`state` == `outputs[-1]`\n",
    "\n",
    "\n",
    "```python\n",
    "# create a BasicRNNCell\n",
    "rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)\n",
    "\n",
    "# 'outputs' is a tensor of shape [batch_size, max_time, cell_state_size]\n",
    "\n",
    "# defining initial state\n",
    "initial_state = rnn_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "# 'state' is a tensor of shape [batch_size, cell_state_size]\n",
    "outputs, state = tf.nn.dynamic_rnn(rnn_cell, input_data,\n",
    "                                   initial_state=initial_state,\n",
    "                                   dtype=tf.float32)\n",
    "```\n",
    "\n",
    "```python\n",
    "# create 2 LSTMCells\n",
    "rnn_layers = [tf.nn.rnn_cell.LSTMCell(size) for size in [128, 256]]\n",
    "\n",
    "# create a RNN cell composed sequentially of a number of RNNCells\n",
    "multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)\n",
    "\n",
    "# 'outputs' is a tensor of shape [batch_size, max_time, 256]\n",
    "# 'state' is a N-tuple where N is the number of LSTMCells containing a\n",
    "# tf.contrib.rnn.LSTMStateTuple for each cell\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=multi_rnn_cell,\n",
    "                                   inputs=data,\n",
    "                                   dtype=tf.float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
