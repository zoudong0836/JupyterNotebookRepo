{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#循环神经网络-(recurrent-neural-network,-RNN)\" data-toc-modified-id=\"循环神经网络-(recurrent-neural-network,-RNN)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>循环神经网络 (recurrent neural network, RNN)</a></span></li><li><span><a href=\"#随时间反向传播-(BPTT)\" data-toc-modified-id=\"随时间反向传播-(BPTT)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>随时间反向传播 (BPTT)</a></span></li><li><span><a href=\"#长短时记忆网络-(LTSM)\" data-toc-modified-id=\"长短时记忆网络-(LTSM)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>长短时记忆网络 (LTSM)</a></span><ul class=\"toc-item\"><li><span><a href=\"#遗忘门\" data-toc-modified-id=\"遗忘门-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>遗忘门</a></span></li><li><span><a href=\"#输入门\" data-toc-modified-id=\"输入门-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>输入门</a></span></li><li><span><a href=\"#输出门\" data-toc-modified-id=\"输出门-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>输出门</a></span></li></ul></li><li><span><a href=\"#循环神经网络的变种\" data-toc-modified-id=\"循环神经网络的变种-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>循环神经网络的变种</a></span><ul class=\"toc-item\"><li><span><a href=\"#双向循环神经网络-(bidirectional-RNN)\" data-toc-modified-id=\"双向循环神经网络-(bidirectional-RNN)-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>双向循环神经网络 (bidirectional RNN)</a></span></li><li><span><a href=\"#深层循环神经网络-(deepRNN)\" data-toc-modified-id=\"深层循环神经网络-(deepRNN)-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>深层循环神经网络 (deepRNN)</a></span></li><li><span><a href=\"#循环神经网络的dropout\" data-toc-modified-id=\"循环神经网络的dropout-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>循环神经网络的dropout</a></span></li></ul></li><li><span><a href=\"#利用RNN训练手写数字MNIST数据集\" data-toc-modified-id=\"利用RNN训练手写数字MNIST数据集-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>利用RNN训练手写数字MNIST数据集</a></span><ul class=\"toc-item\"><li><span><a href=\"#tf.nn.dynamic_rnn-函数说明\" data-toc-modified-id=\"tf.nn.dynamic_rnn-函数说明-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>tf.nn.dynamic_rnn 函数说明</a></span></li></ul></li><li><span><a href=\"#利用RNN进行回归训练\" data-toc-modified-id=\"利用RNN进行回归训练-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>利用RNN进行回归训练</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环神经网络 (recurrent neural network, RNN)\n",
    "[Link1](https://www.imooc.com/article/23821)\n",
    "\n",
    "\n",
    "> 循环神经网络的主要用途是处理和预测`序列数据`。循环神经网络的隐藏层之间的结点是有连接的，隐藏层的输入不仅包括输入层的输出，还包括上一时刻隐藏层的输出。\n",
    "\n",
    "> 循环神经网络会对于每一个时刻的输入结合当前模型的状态给出一个输出。循环神经网络在每一个时刻会有一个输入X(t)，然后根据循环神经网络当前的状态A(t)提供一个输出H(t)；而循环神经网络当前的状态A(t)是根据上一时刻的状态A(t-1)和当前的输入X(t)共同决定的。\n",
    "\n",
    "> 对于一个序列数据，可以将这个序列上不同时刻的数据依次传入循环神经网络的输入层，而输出可以是对序列中下一个时刻的预测，也可以是对当前时刻信息的处理结果。\n",
    "\n",
    "> 在循环神经网络中，循环体网络结构中的参数`在不同时刻是共享的`。\n",
    "\n",
    "> 循环神经网络的总损失为所有时刻(或者部分时刻)上的损失函数的总和。\n",
    "\n",
    "![RNN前向传播计算示意图](illustration/RNN前向传播计算示意图.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随时间反向传播 (BPTT)\n",
    "由于RNN是一种基于时序数据的神经网络模型，因此传统的BP算法并不适用于该模型的优化；在RNN中最常用的优化算法是`随时间反向传播`(BackPropagation Through Time)  \n",
    "\n",
    "<br/>在RNN网络中，总体错误是每一时刻的错误的累加和。我们的目标是计算错误值相对于参数U, V, W的梯度以及用随机梯度下降学习好的参数；为了计算梯度，使用的是链式求导法则，主要是用反向传播算法往后传播错误。\n",
    "\n",
    "![BPTT示意图](illustration/BPTT示意图.png)\n",
    "\n",
    "> * 绿色箭头表示的是时间上的反向传播过程\n",
    "> * 红色箭头表示的是同一时刻空间上的传播过程 (也就是普通前馈神经网络的误差过程)\n",
    "\n",
    "BPTT算法存在的问题\n",
    "*  梯度消失 (gradient vanishing）\n",
    ">  有效的解决方案有：合适的初始化矩阵W可以减小梯度消失效应，正则化也能起作用。更好的方法是选择ReLU而不是sigmoid和tanh作为激活函数。ReLU的导数是常数值0或1，所以不可能会引起梯度消失。更通用的方案时采用长短项记忆（LSTM）或门限递归单元（GRU）结构  \n",
    "> LSTM只能避免RNN的梯度消失（gradient vanishing）\n",
    "\n",
    "*  梯度爆炸 (gradient explosion)\n",
    ">  一般靠裁剪后的优化算法即可解决，比如gradient clipping（如果梯度的范数大于某个给定值，将梯度同比收缩）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 长短时记忆网络 (LTSM)\n",
    "\n",
    "循环神经网络工作的关键点是使用历史的信息来帮助当前的决策；但同时也带来了更大的技术挑战 -- 长期依赖(long-term dependencies)\n",
    "\n",
    "<br/>LSTM的提出就是为了解决长期依赖问题。LSTM是一种拥有三个\"门\"结构的特殊网络结构。(所谓\"门\"的结构就是一个使用sigmoid神经网络和和一个按位做乘法的操作，这两个操作合在一起就是一个\"门\"结构，之所以该结构叫做\"门\"是因为使用sigmoid作为激活函数的全连接神经网络层会输出一个0到1之间的数值，描述当前输入有多少信息量可以通过这个结构)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 遗忘门\n",
    "LSTM 中的第一步是决定会从细胞状态中丢弃什么信息。这个决定通过一个称为忘记门层完成。该门会读取`h(t-1)`和`x(t)`，输出一个在 0到 1之间的数值给每个在细胞状态 `C(t-1)` 中的数字。1 表示“完全保留”，0 表示“完全舍弃”。\n",
    "![forget_gate](illustration/forget_gate.jpg)\n",
    "\n",
    "> 其中`h(t-1)`表示的是上一个cell的输出, `x(t)`表示的是当前cell的输入, σ 表示sigmod函数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入门\n",
    "LSTM 中的第二步是决定让多少新的信息加入到 cell 状态中来。实现这个需要包括两个步骤：首先，一个叫做“input gate layer ”的 sigmoid 层决定哪些信息需要更新；一个 tanh 层生成一个向量，也就是备选的用来更新的内容。在下一步，把这两部分联合起来，对 cell 的状态进行一个更新\n",
    "![input_gate](illustration/input_gate.jpg)\n",
    "\n",
    "<br/>现在是更新旧细胞状态的时间了，目标是将`C(t-1)`更新为`C(t)`。 把旧状态与`f(t)`相乘，丢弃掉确定需要丢弃的信息；接着加上`i(t)` * `C^(t)`，这就是新的候选值，这两者决定了每个状态更新情况\n",
    "![input_gate_02](illustration/input_gate_02.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出门\n",
    "LSTM 中的第三步是决定输出什么值。这个输出将会基于 cell 的状态，但是也是一个过滤后的版本。首先，运行一个 sigmoid 层来确定细胞状态的哪个部分将输出出去。接着，把细胞状态通过 tanh 进行处理（得到一个在 -1 到 1 之间的值）并将它和 sigmoid 门的输出相乘，最终仅仅会输出确定输出的那部分。\n",
    "![output_gate](illustration/output_gate.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个LSTM结构, 并指定隐藏层的大小\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_hidden_size)\n",
    "'''\n",
    "__init__(\n",
    "    num_units,\n",
    "    forget_bias=1.0,\n",
    "    state_is_tuple=True,\n",
    "    activation=None,\n",
    "    reuse=None,\n",
    "    name=None,\n",
    "    dtype=None\n",
    ")\n",
    "'''\n",
    "\n",
    "# 将LSTM中的状态初始化为全0数组. 和其他神经网络类似, 在优化神经网络时, \n",
    "# 每次也会使用一个batch的训练样本, 以下代码中, batch_size给出了一个batch大小\n",
    "# BasicLSTMCell类提供了zero_state函数来生成全0的初始状态\n",
    "state = lstm.zero_state(batch_size, tf.float32)\n",
    "'''\n",
    "zero_state(batch_size, dtype)\n",
    "Args:\n",
    "    batch_size:  int, float, or unit Tensor representing the batch size.\n",
    "    dtype:  the data type to use for the state.\n",
    "Return:\n",
    "    zero-filled state tensor(s).    \n",
    "'''\n",
    "\n",
    "# 虽然理论上循环神经网络可以处理任意长度的序列, 但是在训练时为了避免梯度消散的问题,\n",
    "# 通常会规定一个最大的序列长度, 以下代码中, 用 num_steps 来表示这个长度\n",
    "for i in range(num_steps):\n",
    "    # 在第一个时刻声明LSTM结构中使用的变量\n",
    "    # 在之后的时刻都需要复用之前定义好的变量\n",
    "    if i > 0:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    # 每一步处理时间序列中的一个时刻。将当前输入(current_input)和前一时刻状态(state)\n",
    "    # 传入定义的LSTM结构可以得到当前LSTM结构的输出lstm_output和更新后的状态state\n",
    "    lstm_output, state = lstm(current_input, state)\n",
    "    # 将当前时刻LSTM结构的输出传入一个全连接层得到最后的输出\n",
    "    final_output = fully_connected(lstm_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环神经网络的变种"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 双向循环神经网络 (bidirectional RNN)\n",
    "\n",
    "双向循环神经网络是由两个循环神经网络上下叠加在一起组成的。输出由这两个循环神经网络的状态共同决定。  \n",
    "![双向循环神经网络结构示意图](illustration/双向循环神经网络结构示意图.png)  \n",
    "\n",
    "> 双向循环神经网络的主体结构就是两个单向循环神经网络的结合。在每一个时刻`t`, 输入会同时提供给这两个方向相反的循环神经网络, 而输出则是由这两个单向循环神经网络共同决定。双向循环神经网络的前向传播过程和单向的循环神经网络十分类似"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深层循环神经网络 (deepRNN)\n",
    "深层循环神经网络在每一个时刻上将循环体结构复制了多次，每一层的循环体中参数是一致的，而不同层中的参数可以不同   \n",
    "![深层循环神经网络结构示意图](illustration/深层循环神经网络结构示意图.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 循环神经网络的dropout\n",
    "在循环神经网络中使用`dropout`可以使网络更加健壮；区别于卷积神经网络只在最后的全连接层中使用`dropout`, 循环神经网络一般只在不同层循环体结构之间使用`dropout`, 而不在同一层的循环体结构之间使用。也就是说从时刻`t-1`传递到时刻`t`时，循环神经网络不会进行状态的`dropout`；而在同一时刻`t`中，不同层循环体之间会使用`dropout`  \n",
    "![深层循环神经网络使用dropout示意图](illustration/深层循环神经网络使用dropout示意图.png)\n",
    "\n",
    "> 图中实线箭头表示不使用`dropout`，虚线箭头表示使用`dropout`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用RNN训练手写数字MNIST数据集\n",
    "[Link1](https://blog.csdn.net/omnispace/article/details/78415100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.set_random_seed(1)   # set random seed\n",
    " \n",
    "# 导入数据\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    " \n",
    "# hyperparameters\n",
    "lr = 0.001                         # learning rate\n",
    "training_iters = 100000     # train step 上限\n",
    "batch_size = 128            \n",
    "n_inputs = 28                  # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28                   # time steps\n",
    "n_hidden_units = 128       # neurons in hidden layer\n",
    "n_classes = 10                # MNIST classes (0-9 digits)\n",
    "\n",
    "# x y placeholder\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    " \n",
    "# 对 weights biases 初始值的定义\n",
    "weights = {\n",
    "    # shape (28, 128)\n",
    "    'in': tf.Variable(tf.random_normal([n_inputs, n_hidden_units])),\n",
    "    # shape (128, 10)\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_units, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    # shape (128, )\n",
    "    'in': tf.Variable(tf.constant(0.1, shape=[n_hidden_units, ])),\n",
    "    # shape (10, )\n",
    "    'out': tf.Variable(tf.constant(0.1, shape=[n_classes, ]))\n",
    "}\n",
    "\n",
    "'''\n",
    "  RNN 主体总共有3部分组成(input_layer[输入隐藏层],  rnn_cell[rnn网络层],  output_layer[输出隐藏层])\n",
    "'''\n",
    "def RNN(X, weights, biases):\n",
    "    # 定义 input_layer 层...\n",
    "    # 原始的 X 是 3 维数据, 我们需要把它变成 2 维数据才能使用 weights 的矩阵乘法\n",
    "    # X ==> (128 batches * 28 steps, 28 inputs)\n",
    "    X = tf.reshape(X, [-1, n_inputs])\n",
    "    # X_in = W*X + b\n",
    "    X_in = tf.matmul(X, weights['in']) + biases['in']\n",
    "    # X_in ==> (128 batches, 28 steps, 128 hidden) 换回3维\n",
    "    X_in = tf.reshape(X_in, [-1, n_steps, n_hidden_units])\n",
    "\n",
    "    # 定义 rnn_cell 层...\n",
    "    # 使用 basic LSTM Cell.\n",
    "    # 对于 lstm 来说, state可被分为(c_state, h_state).\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_units, forget_bias=1.0, state_is_tuple=True)\n",
    "    # 初始化全零 state\n",
    "    init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32) \n",
    "    # 如果 inputs 为 (batches, steps, inputs) ==> time_major=False;\n",
    "    # 如果 inputs 为 (steps, batches, inputs) ==> time_major=True;\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, X_in, initial_state=init_state, time_major=False)\n",
    "\n",
    "    # 定义 output_layer 层...\n",
    "    # 由于该例的特殊性, 有两种方法来计算 output_layer 输出\n",
    "    # 方式一: 直接调用final_state 中的 h_state ( 通过final_state[1]获取) 来进行运算:\n",
    "    results = tf.matmul(final_state[1], weights['out']) + biases['out']  \n",
    "    # 方式二: 调用最后一个 outputs (在这个例子中, 和上面的final_state[1]是一样的):\n",
    "    # 由于time_major=False,  故outputs 的形状为 [batch_size, max_time, cell.output_size]\n",
    "    # 把 outputs 变成 列表 [(batch, outputs)..] * steps\n",
    "    outputs = tf.unstack(tf.transpose(outputs, [1,0,2]))\n",
    "    results = tf.matmul(outputs[-1], weights['out']) + biases['out']    #选取最后一个 output\n",
    "\n",
    "return results\n",
    "\n",
    "# 定义 cost 和 train_op:\n",
    "pred = RNN(x, weights, biases)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "train_op = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "\n",
    "# 训练 RNN\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "init = tf.global_variables_initializer()\n",
    " \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 0\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        batch_xs = batch_xs.reshape([batch_size, n_steps, n_inputs])\n",
    "        sess.run([train_op], feed_dict={\n",
    "            x: batch_xs,\n",
    "            y: batch_ys,\n",
    "        })\n",
    "        if step % 20 == 0:\n",
    "            print(sess.run(accuracy, feed_dict={\n",
    "            x: batch_xs,\n",
    "            y: batch_ys,\n",
    "        }))\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.nn.dynamic_rnn 函数说明\n",
    "\n",
    "```python\n",
    "tf.nn.dynamic_rnn(\n",
    "    cell,\n",
    "    inputs,\n",
    "    sequence_length=None,\n",
    "    initial_state=None,\n",
    "    dtype=None,\n",
    "    parallel_iterations=None,\n",
    "    swap_memory=False,\n",
    "    time_major=False,\n",
    "    scope=None\n",
    ")\n",
    "```\n",
    "\n",
    "Returns:   A pair (outputs, state)\n",
    "[Link1](https://blog.csdn.net/u010960155/article/details/81707498)\n",
    "\n",
    "outputs 是一个tensor  \n",
    "> 如果time_major==True，outputs形状为 [max_time, batch_size, cell.output_size]（要求rnn输入与rnn输出形状保持一致）     \n",
    "\n",
    "> 如果time_major==False(默认)，outputs形状为 [batch_size, max_time, cell.output_size]\n",
    "   \n",
    "\n",
    "state 是一个tensor  \n",
    "> 如果cell为LSTM, state形状为[2，batch_size, cell.output_size]; 每个cell会有两个输出: `C(t)` 和 `h(t)`，上面这个图是输出`C(t)`，代表哪些信息应该被记住哪些应该被遗忘；下面这个图是输出`h(t)`，代表这个cell的最终输出，LSTM的state是由`C(t)` 和 `h(t)` 组成的\n",
    "![LSTM_state_示意图](illustration/LSTM_state_示意图.png)\n",
    "\n",
    "> 如果cell为GRU, state形状为[batch_size, cell.output_size];  state就只有一个了，原因是GRU将`C(t)` 和 `h(t)` 进行了简化，将其合并成了`h(t)`，如下图所示，GRU将遗忘门和输入门合并成了更新门，另外cell不在有细胞状态cell state，只有hidden state\n",
    "![GRU_state_示意图](illustration/GRU_state_示意图.png)\n",
    "\n",
    "output 与 state 的关系\n",
    "> 如果cell为LSTM，那 state是个tuple，分别代表`C(t)` 和 `h(t)` ，其中 `h(t)` 与outputs中的对应的最后一个时刻的输出相等，假设state形状为[2，batch_size, cell.output_size]，outputs形状为 [batch_size, max_time, cell.output_size]，那么`state[1, batch_size, : ]` == `outputs[batch_size, -1, : ]`\n",
    "```python\n",
    "'''\n",
    "由于time_major=False,  故outputs 的形状为 [batch_size, max_time, cell.output_size]\n",
    "需要将outputs转换成[max_time, batch_size, cell.output_size]形状 [0, 1, 2] => [1, 0, 2]\n",
    "则：outputs[-1] => [ -1, batch_size , cell.output_size ] => [ -1, : , : ] 即为最后一个cell的输出\n",
    "'''\n",
    "outputs = tf.unstack(tf.transpose(outputs, [1,0,2]))\n",
    "results = tf.matmul(outputs[-1], weights['out']) + biases['out'] \n",
    "```\n",
    "\n",
    "> 如果cell为GRU，同上，state其实就是 `h(t)`，`state` == `outputs[-1]`\n",
    "\n",
    "\n",
    "Create a BasicRNNCell\n",
    "```python\n",
    "# create a BasicRNNCell\n",
    "rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)\n",
    "\n",
    "# 'outputs' is a tensor of shape [batch_size, max_time, cell_state_size]\n",
    "\n",
    "# defining initial state\n",
    "initial_state = rnn_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "# 'state' is a tensor of shape [batch_size, cell_state_size]\n",
    "outputs, state = tf.nn.dynamic_rnn(rnn_cell, input_data,\n",
    "                                   initial_state=initial_state,\n",
    "                                   dtype=tf.float32)\n",
    "```\n",
    "\n",
    "Create 2 LSTMCells\n",
    "```python\n",
    "# create 2 LSTMCells\n",
    "rnn_layers = [tf.nn.rnn_cell.LSTMCell(size) for size in [128, 256]]\n",
    "\n",
    "# create a RNN cell composed sequentially of a number of RNNCells\n",
    "multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)\n",
    "\n",
    "# 'outputs' is a tensor of shape [batch_size, max_time, 256]\n",
    "# 'state' is a N-tuple where N is the number of LSTMCells containing a\n",
    "# tf.contrib.rnn.LSTMStateTuple for each cell\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=multi_rnn_cell,\n",
    "                                   inputs=data,\n",
    "                                   dtype=tf.float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def dynamic_rnn(rnn_type='lstm'):\n",
    "\n",
    "    # 创建输入数据, 3代表batch size, 6代表输入序列的最大步长(max time), 4代表每个序列的维度\n",
    "    X = np.random.randn(3, 6, 4) # 返回一个[3, 6, 4]的矩阵\n",
    "\n",
    "    # 将X[1, 4:]的元素置0，即将第二个输入的实际长度修改为4\n",
    "    X[1, 4:] = 0\n",
    "\n",
    "    # 记录三个输入的实际步长 (模拟变长情况)\n",
    "    X_lengths = [6, 4, 6]\n",
    "\n",
    "    # 指定RNN隐藏层神经元个数\n",
    "    rnn_hidden_size = 5\n",
    "\n",
    "    if rnn_type == 'lstm':\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(num_units=rnn_hidden_size)\n",
    "    else:\n",
    "        cell = tf.contrib.rnn.GRUCell(num_units=rnn_hidden_size)\n",
    "\n",
    "    outputs, last_states = tf.nn.dynamic_rnn(\n",
    "        cell = cell,\n",
    "        dtype = tf.float64,\n",
    "        sequence_length = X_lengths,\n",
    "        inputs = X,\n",
    "    )\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        val_output, val_states = sess.run([outputs, last_states])\n",
    "        print np.shape(val_output)       # LSTM: (3, 6, 5);  GRU: (3, 6, 5)\n",
    "        print np.shape(val_states)       # LSTM: (2, 6, 5);  GRU: (3, 5)\n",
    "\n",
    "\"\"\"\n",
    "cell类型为LSTM时，输入的形状为 [3, 6, 4]，经过 `tf.nn.dynamic_rnn` 后outputs的形状为[3, 6, 5]，\n",
    "state形状为 [2, 3, 5], 其中state第一部分为c, 代表 `cell state`; 第二部分为h，代表 `hidden state`。\n",
    "可以看到 `hidden state` 与对应的 `outputs` 的最后一行是相等的。\n",
    "另外需要注意的是输入一共有三个序列，但第二个序列的长度只有4，可以看到outputs中对应的两行值都为0，\n",
    "所以hidden state对应的是最后一个不为0的部分。tf.nn.dynamic_rnn通过设置`sequence_length`来实现这一逻辑\n",
    "\n",
    "\n",
    "cell类型为GRU时，输入的形状为 [3, 6, 4]，经过 `tf.nn.dynamic_rnn` 后outputs的形状为 [3, 6, 5]，\n",
    "state形状为 [3, 5]。可以看到 state 与对应的 outputs 的最后一行是相等的\n",
    "\"\"\"                \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dynamic_rnn('lstm')    \n",
    "    dynamic_rnn('gru')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用RNN进行回归训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding:utf-8 -*-\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "BATCH_START = 0\n",
    "TIME_STEPS = 20\n",
    "BATCH_SIZE = 50\n",
    "INPUT_SIZE = 1\n",
    "OUTPUT_SIZE = 1\n",
    "CELL_SIZE = 10      # LSTM神经元个数\n",
    "LR = 0.006\n",
    "\n",
    "\n",
    "def get_batch():\n",
    "    global BATCH_START, TIME_STEPS\n",
    "    # xs shape (50batch, 20steps)\n",
    "    xs = np.arange(BATCH_START, BATCH_START+TIME_STEPS*BATCH_SIZE).reshape((BATCH_SIZE, TIME_STEPS)) / (10*np.pi)\n",
    "    seq = np.sin(xs)\n",
    "    res = np.cos(xs)\n",
    "    BATCH_START += TIME_STEPS\n",
    "    # seq[:, :, np.newaxis].shape = (50, 20, 1)\n",
    "    # res[:, :, np.newaxis].shape = (50, 20, 1)\n",
    "    return [seq[:, :, np.newaxis], res[:, :, np.newaxis], xs]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LSTMRNN(object):\n",
    "\n",
    "    def __init__(self, n_steps, input_size, output_size, cell_size, batch_size):\n",
    "        self.n_steps = n_steps\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.cell_size = cell_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.l_in_y = None\n",
    "        self.cell_init_state = None\n",
    "        self.cell_outputs = None\n",
    "        self.cell_final_state = None\n",
    "        self.pred = None\n",
    "        self.cost = None\n",
    "\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.xs = tf.placeholder(tf.float32, [None, n_steps, input_size], name='xs')\n",
    "            self.ys = tf.placeholder(tf.float32, [None, n_steps, output_size], name='ys')\n",
    "        with tf.variable_scope('in_hidden'):\n",
    "            self.add_input_layer()\n",
    "        with tf.variable_scope('LSTM_cell'):\n",
    "            self.add_lstm_cell()\n",
    "        with tf.variable_scope('out_hidden'):\n",
    "            self.add_output_layer()\n",
    "        with tf.name_scope('cost'):\n",
    "            self.compute_cost()\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(LR).minimize(self.cost)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def ms_error(labels, logits):\n",
    "        return tf.square(tf.subtract(labels, logits))\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_variable(shape, name='weights'):\n",
    "        initializer = tf.random_normal_initializer(mean=0., stddev=1.,)\n",
    "        return tf.get_variable(shape=shape, initializer=initializer, name=name)\n",
    "\n",
    "    @staticmethod\n",
    "    def bias_variable(shape, name='biases'):\n",
    "        initializer = tf.constant_initializer(0.1)\n",
    "        return tf.get_variable(name=name, shape=shape, initializer=initializer)\n",
    "\n",
    "\n",
    "\n",
    "    def add_input_layer(self):\n",
    "\n",
    "        # 将3D数据转换成2D, 用于输入隐藏层(2D)计算\n",
    "        l_in_x = tf.reshape(self.xs, [-1, self.input_size], name='2_2D')\n",
    "        Ws_in = self.weight_variable([self.input_size, self.cell_size])\n",
    "        bs_in = self.bias_variable([self.cell_size,])\n",
    "\n",
    "        with tf.name_scope(\"Wx_plus_b\"):\n",
    "            l_in_y = tf.matmul(l_in_x, Ws_in) + bs_in\n",
    "\n",
    "        # 将计算结果转换为3D, 用于lstm计算\n",
    "        self.l_in_y = tf.reshape(l_in_y, [-1, self.n_steps, self.cell_size], name='2_3D')\n",
    "\n",
    "\n",
    "    def add_lstm_cell(self):\n",
    "\n",
    "        lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.cell_size)\n",
    "\n",
    "        with tf.name_scope(\"initial_state\"):\n",
    "            self.cell_init_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "\n",
    "        self.cell_outputs, self.cell_final_state = tf.nn.dynamic_rnn(\n",
    "            lstm_cell, self.l_in_y, initial_state=self.cell_init_state, time_major=False\n",
    "        )\n",
    "\n",
    "\n",
    "    def add_output_layer(self):\n",
    "\n",
    "        # 将3D数据转换成2D, 用于输入隐藏层(2D)计算\n",
    "        # time_major=False => self.cell_outputs = [batch_size, max_time, cell.output_size]\n",
    "        # l_out_x = [(batch_size * max_time), cell.output_size]\n",
    "        # 注意区别 (计算损失函数的不同导致 outputs 的取值变化)\n",
    "        # outputs = tf.unstack(tf.transpose(outputs, [1,0,2]))\n",
    "        # tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "        l_out_x = tf.reshape(self.cell_outputs, [-1, self.cell_size], name='2_2D')\n",
    "        Ws_out = self.weight_variable([self.cell_size, self.output_size])\n",
    "        bs_out = self.bias_variable([self.output_size, ])\n",
    "\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            # np.shape(self.pred) = (1000, 1) => [(BATCH_SIZE * TIME_STEPS), OUTPUT_SIZE]\n",
    "            self.pred = tf.matmul(l_out_x, Ws_out) + bs_out\n",
    "\n",
    "\n",
    "    def compute_cost(self):\n",
    "\n",
    "        \"\"\"\n",
    "            tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "                logits,\n",
    "                targets,\n",
    "                weights,\n",
    "                average_across_timesteps=True,\n",
    "                softmax_loss_function=None,\n",
    "                name=None\n",
    "            )\n",
    "\n",
    "            # Weighted cross-entropy loss for a sequence of logits (per example).\n",
    "            # 这个函数用于计算所有example的加权交叉熵损失\n",
    "            # 该函数的返回值是一个1D float类型的Tensor，尺寸为`batch_size`，\n",
    "            # 其中的每一个元素代表当前输入序列example的交叉熵\n",
    "\n",
    "            # logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].\n",
    "            # targets: List of 1D batch-sized int32 Tensors of the same length as logits\n",
    "            # weights: List of 1D batch-sized float-Tensors of the same length as logits.\n",
    "            # average_across_timesteps: If set, divide the returned cost by the total label weight.\n",
    "            # softmax_loss_function: Function (labels, logits) ->\n",
    "                    loss-batch to be used instead of the standard softmax (the default if this is None).\n",
    "\n",
    "            # 结合本例进一步理解:\n",
    "            #   logits 的shape = [(batch_size * time_step), output_size], output_size 即使(分类)类别的个数\n",
    "            #   targets 的shape = [(batch_size * time_step)]\n",
    "            #   sequence_loss_by_example 的做法是, 针对logits中的每一个`time_step`, 即[batch_size, output_size],\n",
    "            # 对所有`output_size`个预测结果, 得出预测值最大的那个类别, 与target中的值相比较计算`loss`值\n",
    "        \"\"\"\n",
    "\n",
    "        losses = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "            [tf.reshape(self.pred, [-1], name='reshape_pred')],    # shape = (1000, )\n",
    "            [tf.reshape(self.ys, [-1], name='reshape_target')],    # shape = (1000, ）\n",
    "            [tf.ones([self.batch_size * self.n_steps], dtype=tf.float32)],\n",
    "            average_across_timesteps = True,\n",
    "            softmax_loss_function = self.ms_error,\n",
    "            name = 'losses'\n",
    "        )\n",
    "\n",
    "        # 对`losses`求均值, 返回一个标量\n",
    "        with tf.name_scope('average_cost'):\n",
    "            self.cost = tf.div(\n",
    "                tf.reduce_mean(losses, name='losses_sum'),\n",
    "                self.batch_size,\n",
    "                name = 'average_cost'\n",
    "            )\n",
    "            tf.summary.scalar('cost', self.cost)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    model = LSTMRNN(TIME_STEPS, INPUT_SIZE, OUTPUT_SIZE, CELL_SIZE, BATCH_SIZE)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"logs\", sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    plt.ion()\n",
    "    plt.show()\n",
    "\n",
    "    state = None\n",
    "    for i in range(10):\n",
    "        seq, res, xs = get_batch()\n",
    "        if i == 0:\n",
    "            feed_dict = {model.xs : seq, model.ys : res}\n",
    "        else:\n",
    "            # 动态更新 tf.nn.dynamic_rnn 函数的 initial_state\n",
    "            feed_dict = {model.xs : seq, model.ys : res, model.cell_init_state : state}\n",
    "\n",
    "        _, cost, state, pred = sess.run(\n",
    "            [model.train_op, model.cost, model.cell_final_state, model.pred],\n",
    "            feed_dict = feed_dict\n",
    "        )\n",
    "\n",
    "        plt.plot(xs[0, :], res[0].flatten(), 'r', xs[0, :], pred.flatten()[:TIME_STEPS], 'b--')\n",
    "        plt.ylim((-1.2, 1.2))\n",
    "        plt.draw()\n",
    "        plt.pause(0.3)\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            print 'cost: ', round(cost, 4)\n",
    "            result = sess.run(merged, feed_dict)\n",
    "            writer.add_summary(result, i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
